{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, BatchNorm, global_add_pool, ChebConv, global_max_pool, SAGPooling, GATConv, GATv2Conv, TransformerConv, SuperGATConv, global_mean_pool, Linear\n",
    "from torch.nn import BatchNorm1d\n",
    "from math import floor\n",
    "import torch\n",
    "import torch_geometric\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGConvNetMini(nn.Module):\n",
    "    \"\"\"Same as EEGGraphConvNet but with fewer \n",
    "    convolutional layers\n",
    "    \"\"\"\n",
    "    def __init__(self, reduced_sensors=True, sfreq=None, batch_size=256, input_size=6):\n",
    "        super(EEGConvNetMini, self).__init__()\n",
    "        # Define and initialize hyperparameters\n",
    "        self.sfreq = sfreq\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Layers definition\n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(self.input_size, 16, cached=True, normalize=False)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(16, 8)\n",
    "        self.fc2 = nn.Linear(8, 4)\n",
    "        self.fc3 = nn.Linear(4, 2)\n",
    "        \n",
    "        # Xavier initializacion for fully connected layers\n",
    "        self.fc1.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc2.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc3.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weigth, batch):\n",
    "        # Perform all graph convolutions\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index, edge_weigth), negative_slope=0.01)\n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Perform batch normalization\n",
    "        x = F.leaky_relu(self.batch_norm(x), negative_slope=0.01)\n",
    "        #x = F.dropout(batch_norm_out, p=0.2, training=self.training)\n",
    "        \n",
    "        # Global add pooling\n",
    "        mean_pool = global_add_pool(x, batch=batch)\n",
    "        \n",
    "        # Apply fully connected layters\n",
    "        out = F.leaky_relu(self.fc1(mean_pool), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        \n",
    "        out = F.leaky_relu(self.fc2(out), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        \n",
    "        out = F.leaky_relu(self.fc3(out))\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corrected_data_list(path):\n",
    "    data_list = list()\n",
    "    for file in path.iterdir():\n",
    "        data_list.append(torch.load(file))\n",
    "    corrected_data_list = list()\n",
    "    for data in data_list:\n",
    "    # print(data)\n",
    "        data = torch_geometric.data.Data(\n",
    "            x=torch.tensor(data.x),\n",
    "            edge_index=torch.tensor(data.edge_index),\n",
    "            edge_attr=torch.tensor(data.edge_attr),\n",
    "            label=torch.tensor(data.label),\n",
    "        )\n",
    "        corrected_data_list.append(data)\n",
    "       \n",
    "       \n",
    "    rm = [\n",
    "      7,\n",
    "      14+1,\n",
    "      14+2,\n",
    "      17+3,\n",
    "      17+4,\n",
    "      26+5,\n",
    "      38+6,\n",
    "      54+7,\n",
    "      65+8,\n",
    "      69+9\n",
    "      ]\n",
    "\n",
    "    dl = list()\n",
    "    start = 0\n",
    "    for r in rm:\n",
    "        dl.extend(corrected_data_list[start:r])\n",
    "        start = r + 1\n",
    "\n",
    "    dl.extend(corrected_data_list[start:])\n",
    "    dl_filterd = list()\n",
    "    for data in dl:\n",
    "        if data.label == 2:\n",
    "            # print(data.label)\n",
    "            # if data.label == 2:\n",
    "            data.label = torch.tensor(1)\n",
    "        dl_filterd.append(data)\n",
    "\n",
    "    len(dl_filterd)\n",
    "    return dl_filterd\n",
    "\n",
    "def print_classification_report(y_pred, y_true):\n",
    "    print(classification_report(y_true, y_pred))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moments Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_31200\\765743231.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/moments_pearson/')\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EEGConvNetMini(input_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dl_filterd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGConvNetMini(\n",
       "  (conv1): GCNConv(6, 16)\n",
       "  (batch_norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (fc2): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (fc3): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=0.695010\t acc=0.653846\n",
      "Test  : epoch=1\t loss=0.968978\t acc=0.562500\n",
      "\n",
      "Train : epoch=2\t loss=0.937180\t acc=0.653846\n",
      "Test  : epoch=2\t loss=2.036176\t acc=0.562500\n",
      "\n",
      "Train : epoch=3\t loss=1.343597\t acc=0.653846\n",
      "Test  : epoch=3\t loss=7.810587\t acc=0.500000\n",
      "\n",
      "Train : epoch=4\t loss=2.027792\t acc=0.653846\n",
      "Test  : epoch=4\t loss=13.934993\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=2.729467\t acc=0.653846\n",
      "Test  : epoch=5\t loss=19.076998\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=3.034702\t acc=0.653846\n",
      "Test  : epoch=6\t loss=19.996005\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=3.058595\t acc=0.653846\n",
      "Test  : epoch=7\t loss=19.979201\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=3.055644\t acc=0.653846\n",
      "Test  : epoch=8\t loss=19.921308\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=3.054728\t acc=0.653846\n",
      "Test  : epoch=9\t loss=19.334044\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=3.066182\t acc=0.653846\n",
      "Test  : epoch=10\t loss=18.738665\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moments PLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_31200\\765743231.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/moments_pli/')\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EEGConvNetMini(input_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGConvNetMini(\n",
       "  (conv1): GCNConv(6, 16)\n",
       "  (batch_norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (fc2): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (fc3): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=0.876429\t acc=0.641026\n",
      "Test  : epoch=1\t loss=36.490707\t acc=0.625000\n",
      "\n",
      "Train : epoch=2\t loss=1.310649\t acc=0.653846\n",
      "Test  : epoch=2\t loss=28.235909\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=1.849130\t acc=0.653846\n",
      "Test  : epoch=3\t loss=22.191071\t acc=0.562500\n",
      "\n",
      "Train : epoch=4\t loss=2.665465\t acc=0.653846\n",
      "Test  : epoch=4\t loss=64.719284\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=3.145089\t acc=0.653846\n",
      "Test  : epoch=5\t loss=93.754211\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=3.406879\t acc=0.653846\n",
      "Test  : epoch=6\t loss=136.812743\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=3.412288\t acc=0.653846\n",
      "Test  : epoch=7\t loss=138.711183\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=3.517989\t acc=0.653846\n",
      "Test  : epoch=8\t loss=93.883008\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=3.300659\t acc=0.653846\n",
      "Test  : epoch=9\t loss=96.688724\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=3.641460\t acc=0.653846\n",
      "Test  : epoch=10\t loss=91.969898\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_31200\\114600516.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x=torch.tensor(data.x),\n",
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_31200\\114600516.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/raw_pearson/')\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EEGConvNetMini(input_size=153550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 2457030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGConvNetMini(\n",
       "  (conv1): GCNConv(153550, 16)\n",
       "  (batch_norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (fc2): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (fc3): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=2.753490\t acc=0.551282\n",
      "Test  : epoch=1\t loss=95.415007\t acc=0.500000\n",
      "\n",
      "Train : epoch=2\t loss=3.404291\t acc=0.653846\n",
      "Test  : epoch=2\t loss=55.725266\t acc=0.437500\n",
      "\n",
      "Train : epoch=3\t loss=2.866058\t acc=0.653846\n",
      "Test  : epoch=3\t loss=78.643512\t acc=0.437500\n",
      "\n",
      "Train : epoch=4\t loss=3.428240\t acc=0.653846\n",
      "Test  : epoch=4\t loss=59.969589\t acc=0.437500\n",
      "\n",
      "Train : epoch=5\t loss=3.118471\t acc=0.653846\n",
      "Test  : epoch=5\t loss=60.295269\t acc=0.375000\n",
      "\n",
      "Train : epoch=6\t loss=2.944756\t acc=0.653846\n",
      "Test  : epoch=6\t loss=29.285337\t acc=0.312500\n",
      "\n",
      "Train : epoch=7\t loss=2.881629\t acc=0.653846\n",
      "Test  : epoch=7\t loss=15.170636\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=2.804063\t acc=0.653846\n",
      "Test  : epoch=8\t loss=11.137017\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=2.685399\t acc=0.653846\n",
      "Test  : epoch=9\t loss=36.964104\t acc=0.312500\n",
      "\n",
      "Train : epoch=10\t loss=2.596020\t acc=0.653846\n",
      "Test  : epoch=10\t loss=68.638241\t acc=0.312500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.38      0.62      0.48         8\n",
      "\n",
      "    accuracy                           0.31        16\n",
      "   macro avg       0.19      0.31      0.24        16\n",
      "weighted avg       0.19      0.31      0.24        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_classification_report(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw PLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_31200\\114600516.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x=torch.tensor(data.x),\n",
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_31200\\114600516.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/raw_pli/')\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EEGConvNetMini(input_size=153550, sfreq=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 2457030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGConvNetMini(\n",
       "  (conv1): GCNConv(153550, 16)\n",
       "  (batch_norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (fc2): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (fc3): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=1.585948\t acc=0.641026\n",
      "Test  : epoch=1\t loss=47.135586\t acc=0.562500\n",
      "\n",
      "Train : epoch=2\t loss=2.149640\t acc=0.653846\n",
      "Test  : epoch=2\t loss=53.580137\t acc=0.625000\n",
      "\n",
      "Train : epoch=3\t loss=2.695424\t acc=0.653846\n",
      "Test  : epoch=3\t loss=59.521744\t acc=0.437500\n",
      "\n",
      "Train : epoch=4\t loss=2.676652\t acc=0.653846\n",
      "Test  : epoch=4\t loss=58.465616\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=2.660961\t acc=0.653846\n",
      "Test  : epoch=5\t loss=50.986474\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=2.677856\t acc=0.653846\n",
      "Test  : epoch=6\t loss=49.414424\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=2.606995\t acc=0.653846\n",
      "Test  : epoch=7\t loss=49.729958\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=2.582192\t acc=0.653846\n",
      "Test  : epoch=8\t loss=49.759071\t acc=0.562500\n",
      "\n",
      "Train : epoch=9\t loss=2.677484\t acc=0.653846\n",
      "Test  : epoch=9\t loss=49.931299\t acc=0.562500\n",
      "\n",
      "Train : epoch=10\t loss=2.562927\t acc=0.653846\n",
      "Test  : epoch=10\t loss=41.128629\t acc=0.562500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.12      0.22         8\n",
      "           1       0.53      1.00      0.70         8\n",
      "\n",
      "    accuracy                           0.56        16\n",
      "   macro avg       0.77      0.56      0.46        16\n",
      "weighted avg       0.77      0.56      0.46        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_classification_report(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSD CSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_31200\\114600516.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/psd_csd/')\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EEGConvNetMini(sfreq=500, input_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGConvNetMini(\n",
       "  (conv1): GCNConv(6, 16)\n",
       "  (batch_norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (fc2): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (fc3): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=0.822290\t acc=0.653846\n",
      "Test  : epoch=1\t loss=0.726788\t acc=0.500000\n",
      "\n",
      "Train : epoch=2\t loss=1.021170\t acc=0.653846\n",
      "Test  : epoch=2\t loss=0.765818\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=1.420816\t acc=0.653846\n",
      "Test  : epoch=3\t loss=1.370760\t acc=0.500000\n",
      "\n",
      "Train : epoch=4\t loss=3.003399\t acc=0.653846\n",
      "Test  : epoch=4\t loss=3.674813\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=3.564597\t acc=0.653846\n",
      "Test  : epoch=5\t loss=4.208704\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=3.563034\t acc=0.653846\n",
      "Test  : epoch=6\t loss=4.221987\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=3.556274\t acc=0.653846\n",
      "Test  : epoch=7\t loss=4.238070\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=3.550967\t acc=0.653846\n",
      "Test  : epoch=8\t loss=4.249033\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=3.545699\t acc=0.653846\n",
      "Test  : epoch=9\t loss=4.256005\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=3.541015\t acc=0.653846\n",
      "Test  : epoch=10\t loss=4.260894\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print_classification_report(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSD Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_31200\\114600516.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/psd_pearson/')\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EEGConvNetMini(sfreq=500, input_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGConvNetMini(\n",
       "  (conv1): GCNConv(6, 16)\n",
       "  (batch_norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (fc2): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (fc3): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=1.663577\t acc=0.282051\n",
      "Test  : epoch=1\t loss=3.404387\t acc=0.625000\n",
      "\n",
      "Train : epoch=2\t loss=0.695628\t acc=0.615385\n",
      "Test  : epoch=2\t loss=1.955927\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=0.688634\t acc=0.653846\n",
      "Test  : epoch=3\t loss=1.480324\t acc=0.562500\n",
      "\n",
      "Train : epoch=4\t loss=0.786435\t acc=0.653846\n",
      "Test  : epoch=4\t loss=0.809998\t acc=0.562500\n",
      "\n",
      "Train : epoch=5\t loss=1.030084\t acc=0.653846\n",
      "Test  : epoch=5\t loss=1.641907\t acc=0.562500\n",
      "\n",
      "Train : epoch=6\t loss=1.709157\t acc=0.653846\n",
      "Test  : epoch=6\t loss=5.226518\t acc=0.562500\n",
      "\n",
      "Train : epoch=7\t loss=2.932782\t acc=0.653846\n",
      "Test  : epoch=7\t loss=9.041891\t acc=0.562500\n",
      "\n",
      "Train : epoch=8\t loss=3.366873\t acc=0.653846\n",
      "Test  : epoch=8\t loss=9.541908\t acc=0.562500\n",
      "\n",
      "Train : epoch=9\t loss=3.342110\t acc=0.653846\n",
      "Test  : epoch=9\t loss=8.950352\t acc=0.562500\n",
      "\n",
      "Train : epoch=10\t loss=3.309732\t acc=0.653846\n",
      "Test  : epoch=10\t loss=9.222539\t acc=0.562500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.12      0.22         8\n",
      "           1       0.53      1.00      0.70         8\n",
      "\n",
      "    accuracy                           0.56        16\n",
      "   macro avg       0.77      0.56      0.46        16\n",
      "weighted avg       0.77      0.56      0.46        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_classification_report(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
