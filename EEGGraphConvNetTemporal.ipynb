{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, BatchNorm, global_add_pool, ChebConv, global_max_pool, SAGPooling, GATConv, GATv2Conv, TransformerConv, SuperGATConv, global_mean_pool, Linear\n",
    "from torch.nn import BatchNorm1d\n",
    "from math import floor\n",
    "import torch\n",
    "import torch_geometric\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGGraphConvNetLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduced_sensors=True, sfreq=None, batch_size=256, **kwargs):\n",
    "        super(EEGGraphConvNetLSTM, self).__init__()\n",
    "        # Define and initialize hyperparameters\n",
    "        self.sfreq = sfreq\n",
    "        self.batch_size = batch_size\n",
    "        #self.input_size = 8 if reduced_sensors else 62\n",
    "        \n",
    "        # Layers definition\n",
    "        input_size = kwargs.pop(\"input_size\", 1280)\n",
    "        hidden_dim = 320\n",
    "        \n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(input_size, 640, cached=True, normalize=False)\n",
    "        self.conv2 = GCNConv(640, 512, cached=True, normalize=False)\n",
    "        self.conv3 = GCNConv(512, 256, cached=True, normalize=False)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm1 = BatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm2 = BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm3 = BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm4 = BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.lstm = nn.LSTM(256, 256, 1, dropout=0.2)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        \n",
    "        # Xavier initializacion for fully connected layers\n",
    "        self.fc1.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc2.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc3.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weigth, batch):\n",
    "        \n",
    "        x = F.leaky_relu(self.batch_norm1(self.conv1(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.batch_norm2(self.conv2(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.batch_norm3(self.conv3(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x, _ = self.lstm(x)\n",
    "        #x = F.leaky_relu(self.batch_norm4(self.conv4(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        #x = F.dropout(batch_norm_out, p=0.2, training=self.training)\n",
    "        \n",
    "        # Global add pooling\n",
    "        add_pool = global_add_pool(x, batch=batch)\n",
    "        # Apply fully connected layters\n",
    "        out = F.leaky_relu(self.fc1(add_pool), negative_slope=0.01)\n",
    "        out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        out = F.leaky_relu(self.fc2(out), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        out = F.leaky_relu(self.fc3(out))\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corrected_data_list(path):\n",
    "    data_list = list()\n",
    "    for file in path.iterdir():\n",
    "        data_list.append(torch.load(file))\n",
    "        \n",
    "    corrected_data_list = list()\n",
    "    \n",
    "    for data in data_list:\n",
    "    # print(data)\n",
    "        data = torch_geometric.data.Data(\n",
    "            x=torch.tensor(data.x),\n",
    "            edge_index=torch.tensor(data.edge_index),\n",
    "            edge_attr=torch.tensor(data.edge_attr),\n",
    "            label=torch.tensor(data.label),\n",
    "        )\n",
    "        corrected_data_list.append(data)\n",
    "        \n",
    "    rm = [\n",
    "      7,\n",
    "      14+1,\n",
    "      14+2,\n",
    "      17+3,\n",
    "      17+4,\n",
    "      26+5,\n",
    "      38+6,\n",
    "      54+7,\n",
    "      65+8,\n",
    "      69+9\n",
    "      ]\n",
    "\n",
    "    dl = list()\n",
    "    start = 0\n",
    "    for r in rm:\n",
    "        dl.extend(corrected_data_list[start:r])\n",
    "        start = r + 1\n",
    "\n",
    "    dl.extend(corrected_data_list[start:])\n",
    "    dl_filterd = list()\n",
    "    for data in dl:\n",
    "        if data.label == 2:\n",
    "            # print(data.label)\n",
    "            # if data.label == 2:\n",
    "            data.label = torch.tensor(1)\n",
    "        dl_filterd.append(data)\n",
    "\n",
    "    len(dl_filterd)\n",
    "    return dl_filterd\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_5624\\129136280.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x=torch.tensor(data.x),\n",
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_5624\\129136280.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[19, 153550], edge_index=[2, 361], edge_attr=[19, 19], label=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('graphs/raw_pearson/')\n",
    "\n",
    "\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=64, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=32, shuffle=False, num_workers=0)\n",
    "train_dl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = EEGGraphConvNetLSTM(input_size = 153550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 99303106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGGraphConvNetLSTM(\n",
       "  (conv1): GCNConv(153550, 640)\n",
       "  (conv2): GCNConv(640, 512)\n",
       "  (conv3): GCNConv(512, 256)\n",
       "  (batch_norm1): BatchNorm(640)\n",
       "  (batch_norm2): BatchNorm(512)\n",
       "  (batch_norm3): BatchNorm(256)\n",
       "  (batch_norm4): BatchNorm(256)\n",
       "  (lstm): LSTM(256, 256, dropout=0.2)\n",
       "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=0.893227\t acc=0.512821\n",
      "Test  : epoch=1\t loss=0.703586\t acc=0.500000\n",
      "\n",
      "Train : epoch=2\t loss=0.595126\t acc=0.602564\n",
      "Test  : epoch=2\t loss=0.706039\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=0.599100\t acc=0.653846\n",
      "Test  : epoch=3\t loss=0.714852\t acc=0.500000\n",
      "\n",
      "Train : epoch=4\t loss=0.371745\t acc=0.666667\n",
      "Test  : epoch=4\t loss=0.963897\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=0.300017\t acc=0.653846\n",
      "Test  : epoch=5\t loss=1.313219\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=0.371194\t acc=0.666667\n",
      "Test  : epoch=6\t loss=1.339589\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=0.341129\t acc=0.666667\n",
      "Test  : epoch=7\t loss=1.377406\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=0.249509\t acc=0.717949\n",
      "Test  : epoch=8\t loss=2.081392\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=0.233442\t acc=0.705128\n",
      "Test  : epoch=9\t loss=3.586349\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=0.244910\t acc=0.692308\n",
      "Test  : epoch=10\t loss=4.043225\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAW PLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_5624\\129136280.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x=torch.tensor(data.x),\n",
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_5624\\129136280.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/raw_pli/')\n",
    "\n",
    "\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=64, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EEGGraphConvNetTemporal(input_size = 153550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 98842562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGGraphConvNetTemporal(\n",
       "  (conv1): GCNConv(153550, 640)\n",
       "  (conv2): GCNConv(640, 512)\n",
       "  (conv3): GCNConv(512, 256)\n",
       "  (conv4): GCNConv(256, 256)\n",
       "  (batch_norm1): BatchNorm(640)\n",
       "  (batch_norm2): BatchNorm(512)\n",
       "  (batch_norm3): BatchNorm(256)\n",
       "  (batch_norm4): BatchNorm(256)\n",
       "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=0.958584\t acc=0.538462\n",
      "Test  : epoch=1\t loss=12.992228\t acc=0.562500\n",
      "\n",
      "Train : epoch=2\t loss=0.885180\t acc=0.589744\n",
      "Test  : epoch=2\t loss=1.125366\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=1.017161\t acc=0.692308\n",
      "Test  : epoch=3\t loss=0.802007\t acc=0.562500\n",
      "\n",
      "Train : epoch=4\t loss=0.523012\t acc=0.692308\n",
      "Test  : epoch=4\t loss=0.591864\t acc=0.437500\n",
      "\n",
      "Train : epoch=5\t loss=0.388701\t acc=0.756410\n",
      "Test  : epoch=5\t loss=1.572760\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=0.547139\t acc=0.692308\n",
      "Test  : epoch=6\t loss=2.747724\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=0.338264\t acc=0.653846\n",
      "Test  : epoch=7\t loss=3.141259\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=0.244018\t acc=0.653846\n",
      "Test  : epoch=8\t loss=6.398689\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=0.293227\t acc=0.653846\n",
      "Test  : epoch=9\t loss=5.214385\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=0.306737\t acc=0.653846\n",
      "Test  : epoch=10\t loss=3.318988\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
