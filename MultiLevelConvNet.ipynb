{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, BatchNorm, global_add_pool, ChebConv, global_max_pool, SAGPooling, GATConv, GATv2Conv, TransformerConv, SuperGATConv, global_mean_pool, Linear\n",
    "from torch.nn import BatchNorm1d\n",
    "from math import floor\n",
    "import torch\n",
    "import torch_geometric\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLevelConvNet(nn.Module):\n",
    "    \"\"\"Same as EEGGraphConvNet but with fewer \n",
    "    convolutional layers\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MultiLevelConvNet, self).__init__()\n",
    "        # Layers definition\n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(153550, 32, cached=True, normalize=False)\n",
    "        self.conv2 = GCNConv(32, 32, cached=True, normalize=False)\n",
    "        self.conv3 = GCNConv(32, 64, cached=True, normalize=False)\n",
    "        \n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm1 = BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm2 = BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm3 = BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(32, 64)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(192, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "        \n",
    "        # Xavier initializacion for fully connected layers\n",
    "        self.fc1.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc2.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc3.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weigth, batch):\n",
    "        x1 = F.leaky_relu(self.batch_norm1(self.conv1(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x2 = F.leaky_relu(self.batch_norm2(self.conv2(x1, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x3 = F.leaky_relu(self.batch_norm3(self.conv3(x2, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        \n",
    "        add_pool1 = global_add_pool(x1, batch=batch)\n",
    "        add_pool2 = global_add_pool(x2, batch=batch)\n",
    "        add_pool3 = global_add_pool(x3, batch=batch)\n",
    "        \n",
    "        out1 = F.leaky_relu(self.fc1(add_pool1), negative_slope=0.01)        \n",
    "        out2 = F.leaky_relu(self.fc2(add_pool2), negative_slope=0.01)        \n",
    "        out3 = F.leaky_relu(self.fc3(add_pool3), negative_slope=0.01)\n",
    "        \n",
    "        out = torch.cat((out1, out2, out3), dim=1)        \n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corrected_data_list(path):\n",
    "    data_list = list()\n",
    "    for file in path.iterdir():\n",
    "        data_list.append(torch.load(file))\n",
    "    corrected_data_list = list()\n",
    "    for data in data_list:\n",
    "    # print(data)\n",
    "        data = torch_geometric.data.Data(\n",
    "            x=torch.tensor(data.x),\n",
    "            edge_index=torch.tensor(data.edge_index),\n",
    "            edge_attr=torch.tensor(data.edge_attr),\n",
    "            label=torch.tensor(data.label),\n",
    "        )\n",
    "        corrected_data_list.append(data)\n",
    "    \n",
    "    rm = [\n",
    "      7,\n",
    "      14+1,\n",
    "      14+2,\n",
    "      17+3,\n",
    "      17+4,\n",
    "      26+5,\n",
    "      38+6,\n",
    "      54+7,\n",
    "      65+8,\n",
    "      69+9\n",
    "      ]\n",
    "\n",
    "    dl = list()\n",
    "    start = 0\n",
    "    for r in rm:\n",
    "        dl.extend(corrected_data_list[start:r])\n",
    "        start = r + 1\n",
    "\n",
    "    dl.extend(corrected_data_list[start:])\n",
    "    dl_filterd = list()\n",
    "    for data in dl:\n",
    "        if data.label == 2:\n",
    "            # print(data.label)\n",
    "            # if data.label == 2:\n",
    "            data.label = torch.tensor(1)\n",
    "        dl_filterd.append(data)\n",
    "\n",
    "    len(dl_filterd)\n",
    "    return dl_filterd\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moments Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_54096\\598882657.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/moments_pearson/')\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLevelConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_filterd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 26530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiLevelConvNet(\n",
       "  (conv1): GCNConv(6, 32)\n",
       "  (conv2): GCNConv(32, 32)\n",
       "  (conv3): GCNConv(32, 64)\n",
       "  (batch_norm1): BatchNorm(32)\n",
       "  (batch_norm2): BatchNorm(32)\n",
       "  (batch_norm3): BatchNorm(64)\n",
       "  (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=192, out_features=64, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=4.607035\t acc=0.538462\n",
      "Test  : epoch=1\t loss=275.223943\t acc=0.500000\n",
      "\n",
      "Train : epoch=2\t loss=5.213089\t acc=0.589744\n",
      "Test  : epoch=2\t loss=1936.248680\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=4.765303\t acc=0.615385\n",
      "Test  : epoch=3\t loss=58.456558\t acc=0.562500\n",
      "\n",
      "Train : epoch=4\t loss=5.065197\t acc=0.653846\n",
      "Test  : epoch=4\t loss=140.867846\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=4.698177\t acc=0.653846\n",
      "Test  : epoch=5\t loss=165.044997\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=3.822331\t acc=0.653846\n",
      "Test  : epoch=6\t loss=163.609787\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=3.844903\t acc=0.653846\n",
      "Test  : epoch=7\t loss=171.055600\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=3.731880\t acc=0.653846\n",
      "Test  : epoch=8\t loss=191.620355\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=3.710398\t acc=0.653846\n",
      "Test  : epoch=9\t loss=179.527705\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=3.742423\t acc=0.653846\n",
      "Test  : epoch=10\t loss=205.975324\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moments PLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_54096\\598882657.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/moments_pli/')\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLevelConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 26530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiLevelConvNet(\n",
       "  (conv1): GCNConv(6, 32)\n",
       "  (conv2): GCNConv(32, 32)\n",
       "  (conv3): GCNConv(32, 64)\n",
       "  (batch_norm1): BatchNorm(32)\n",
       "  (batch_norm2): BatchNorm(32)\n",
       "  (batch_norm3): BatchNorm(64)\n",
       "  (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=192, out_features=64, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=4.567647\t acc=0.576923\n",
      "Test  : epoch=1\t loss=17304.725346\t acc=0.500000\n",
      "\n",
      "Train : epoch=2\t loss=5.922574\t acc=0.653846\n",
      "Test  : epoch=2\t loss=22383.266221\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=4.864628\t acc=0.653846\n",
      "Test  : epoch=3\t loss=29024.502947\t acc=0.500000\n",
      "\n",
      "Train : epoch=4\t loss=4.939911\t acc=0.653846\n",
      "Test  : epoch=4\t loss=35460.005848\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=4.351013\t acc=0.653846\n",
      "Test  : epoch=5\t loss=24331.076101\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=4.911747\t acc=0.653846\n",
      "Test  : epoch=6\t loss=32108.777561\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=4.006101\t acc=0.653846\n",
      "Test  : epoch=7\t loss=18047.943954\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=3.997586\t acc=0.653846\n",
      "Test  : epoch=8\t loss=25389.779441\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=3.748299\t acc=0.653846\n",
      "Test  : epoch=9\t loss=23401.344031\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=3.870325\t acc=0.653846\n",
      "Test  : epoch=10\t loss=27075.311075\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSD CSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_54096\\598882657.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/psd_csd/')\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLevelConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 26530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiLevelConvNet(\n",
       "  (conv1): GCNConv(6, 32)\n",
       "  (conv2): GCNConv(32, 32)\n",
       "  (conv3): GCNConv(32, 64)\n",
       "  (batch_norm1): BatchNorm(32)\n",
       "  (batch_norm2): BatchNorm(32)\n",
       "  (batch_norm3): BatchNorm(64)\n",
       "  (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=192, out_features=64, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=2.269136\t acc=0.653846\n",
      "Test  : epoch=1\t loss=1.690443\t acc=0.500000\n",
      "\n",
      "Train : epoch=2\t loss=3.965007\t acc=0.653846\n",
      "Test  : epoch=2\t loss=2.507696\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=3.955566\t acc=0.653846\n",
      "Test  : epoch=3\t loss=2.515073\t acc=0.500000\n",
      "\n",
      "Train : epoch=4\t loss=3.948190\t acc=0.653846\n",
      "Test  : epoch=4\t loss=2.658419\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=3.963414\t acc=0.653846\n",
      "Test  : epoch=5\t loss=2.900081\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=4.200440\t acc=0.653846\n",
      "Test  : epoch=6\t loss=2.623206\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=3.965634\t acc=0.653846\n",
      "Test  : epoch=7\t loss=2.626941\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=4.032350\t acc=0.653846\n",
      "Test  : epoch=8\t loss=3.397943\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=3.928791\t acc=0.653846\n",
      "Test  : epoch=9\t loss=2.600218\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=3.845010\t acc=0.653846\n",
      "Test  : epoch=10\t loss=2.880694\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSD Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_54096\\598882657.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/psd_pearson/')\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLevelConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 26530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiLevelConvNet(\n",
       "  (conv1): GCNConv(6, 32)\n",
       "  (conv2): GCNConv(32, 32)\n",
       "  (conv3): GCNConv(32, 64)\n",
       "  (batch_norm1): BatchNorm(32)\n",
       "  (batch_norm2): BatchNorm(32)\n",
       "  (batch_norm3): BatchNorm(64)\n",
       "  (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=192, out_features=64, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=4.485378\t acc=0.538462\n",
      "Test  : epoch=1\t loss=44.693730\t acc=0.500000\n",
      "\n",
      "Train : epoch=2\t loss=5.232894\t acc=0.615385\n",
      "Test  : epoch=2\t loss=390.310431\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=4.602283\t acc=0.589744\n",
      "Test  : epoch=3\t loss=199.319306\t acc=0.500000\n",
      "\n",
      "Train : epoch=4\t loss=4.884346\t acc=0.564103\n",
      "Test  : epoch=4\t loss=78.043191\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=5.267218\t acc=0.653846\n",
      "Test  : epoch=5\t loss=128.112685\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=4.420438\t acc=0.653846\n",
      "Test  : epoch=6\t loss=191.420033\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=3.932077\t acc=0.653846\n",
      "Test  : epoch=7\t loss=195.563075\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=3.973272\t acc=0.653846\n",
      "Test  : epoch=8\t loss=226.879498\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=3.819469\t acc=0.653846\n",
      "Test  : epoch=9\t loss=221.417345\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=3.763641\t acc=0.653846\n",
      "Test  : epoch=10\t loss=262.901756\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAW Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_54096\\598882657.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x=torch.tensor(data.x),\n",
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_54096\\598882657.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/raw_pearson/')\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[19, 153550], edge_index=[2, 361], edge_attr=[19, 19], label=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLevelConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 4939938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiLevelConvNet(\n",
       "  (conv1): GCNConv(153550, 32)\n",
       "  (conv2): GCNConv(32, 32)\n",
       "  (conv3): GCNConv(32, 64)\n",
       "  (batch_norm1): BatchNorm(32)\n",
       "  (batch_norm2): BatchNorm(32)\n",
       "  (batch_norm3): BatchNorm(64)\n",
       "  (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=192, out_features=64, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=5.411580\t acc=0.512821\n",
      "Test  : epoch=1\t loss=6.123243\t acc=0.562500\n",
      "\n",
      "Train : epoch=2\t loss=4.787309\t acc=0.602564\n",
      "Test  : epoch=2\t loss=177.158327\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=6.035715\t acc=0.525641\n",
      "Test  : epoch=3\t loss=45.219517\t acc=0.500000\n",
      "\n",
      "Train : epoch=4\t loss=5.331803\t acc=0.564103\n",
      "Test  : epoch=4\t loss=69.755356\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=5.509113\t acc=0.525641\n",
      "Test  : epoch=5\t loss=33.589036\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=4.216980\t acc=0.653846\n",
      "Test  : epoch=6\t loss=61.875588\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=3.680024\t acc=0.653846\n",
      "Test  : epoch=7\t loss=120.918018\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=3.960026\t acc=0.653846\n",
      "Test  : epoch=8\t loss=172.133124\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=3.687382\t acc=0.653846\n",
      "Test  : epoch=9\t loss=209.716006\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=3.711567\t acc=0.653846\n",
      "Test  : epoch=10\t loss=229.410045\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw PLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_54096\\598882657.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x=torch.tensor(data.x),\n",
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_54096\\598882657.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/raw_pli/')\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLevelConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 4939938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiLevelConvNet(\n",
       "  (conv1): GCNConv(153550, 32)\n",
       "  (conv2): GCNConv(32, 32)\n",
       "  (conv3): GCNConv(32, 64)\n",
       "  (batch_norm1): BatchNorm(32)\n",
       "  (batch_norm2): BatchNorm(32)\n",
       "  (batch_norm3): BatchNorm(64)\n",
       "  (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=192, out_features=64, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=3.607204\t acc=0.615385\n",
      "Test  : epoch=1\t loss=3875.349578\t acc=0.500000\n",
      "\n",
      "Train : epoch=2\t loss=4.480674\t acc=0.653846\n",
      "Test  : epoch=2\t loss=6244.634729\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=4.220679\t acc=0.653846\n",
      "Test  : epoch=3\t loss=7125.972633\t acc=0.500000\n",
      "\n",
      "Train : epoch=4\t loss=4.331254\t acc=0.653846\n",
      "Test  : epoch=4\t loss=7132.328710\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=4.103951\t acc=0.653846\n",
      "Test  : epoch=5\t loss=5991.051929\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=4.000556\t acc=0.653846\n",
      "Test  : epoch=6\t loss=6014.055830\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=4.272844\t acc=0.653846\n",
      "Test  : epoch=7\t loss=5923.714538\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=3.896520\t acc=0.653846\n",
      "Test  : epoch=8\t loss=4552.117884\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
