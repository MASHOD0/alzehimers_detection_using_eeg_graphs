{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, BatchNorm, global_add_pool, ChebConv, global_max_pool, SAGPooling, GATConv, GATv2Conv, TransformerConv, SuperGATConv, global_mean_pool, Linear\n",
    "from torch.nn import BatchNorm1d\n",
    "from math import floor\n",
    "import torch\n",
    "import torch_geometric\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLevelConvNet(nn.Module):\n",
    "    \"\"\"Same as EEGGraphConvNet but with fewer \n",
    "    convolutional layers\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MultiLevelConvNet, self).__init__()\n",
    "        # Layers definition\n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(-1, 32, cached=True, normalize=False)\n",
    "        self.conv2 = GCNConv(32, 32, cached=True, normalize=False)\n",
    "        self.conv3 = GCNConv(32, 64, cached=True, normalize=False)\n",
    "        \n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm1 = BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm2 = BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm3 = BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(32, 64)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(192, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "        \n",
    "        # Xavier initializacion for fully connected layers\n",
    "        self.fc1.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc2.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc3.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weigth, batch):\n",
    "        x1 = F.leaky_relu(self.batch_norm1(self.conv1(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x2 = F.leaky_relu(self.batch_norm2(self.conv2(x1, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x3 = F.leaky_relu(self.batch_norm3(self.conv3(x2, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        \n",
    "        add_pool1 = global_add_pool(x1, batch=batch)\n",
    "        add_pool2 = global_add_pool(x2, batch=batch)\n",
    "        add_pool3 = global_add_pool(x3, batch=batch)\n",
    "        \n",
    "        out1 = F.leaky_relu(self.fc1(add_pool1), negative_slope=0.01)        \n",
    "        out2 = F.leaky_relu(self.fc2(add_pool2), negative_slope=0.01)        \n",
    "        out3 = F.leaky_relu(self.fc3(add_pool3), negative_slope=0.01)\n",
    "        \n",
    "        out = torch.cat((out1, out2, out3), dim=1)        \n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corrected_data_list(path):\n",
    "    data_list = list()\n",
    "    for file in path.iterdir():\n",
    "        data_list.append(torch.load(file))\n",
    "    corrected_data_list = list()\n",
    "    for data in data_list:\n",
    "    # print(data)\n",
    "        data = torch_geometric.data.Data(\n",
    "            x=torch.tensor(data.x),\n",
    "            edge_index=torch.tensor(data.edge_index),\n",
    "            edge_attr=torch.tensor(data.edge_attr),\n",
    "            label=torch.tensor(data.label),\n",
    "        )\n",
    "        corrected_data_list.append(data)\n",
    "    \n",
    "    rm = [\n",
    "      7,\n",
    "      14+1,\n",
    "      14+2,\n",
    "      17+3,\n",
    "      17+4,\n",
    "      26+5,\n",
    "      38+6,\n",
    "      54+7,\n",
    "      65+8,\n",
    "      69+9\n",
    "      ]\n",
    "\n",
    "    dl = list()\n",
    "    start = 0\n",
    "    for r in rm:\n",
    "        dl.extend(corrected_data_list[start:r])\n",
    "        start = r + 1\n",
    "\n",
    "    dl.extend(corrected_data_list[start:])\n",
    "    dl_filterd = list()\n",
    "    for data in dl:\n",
    "        if data.label == 2:\n",
    "            # print(data.label)\n",
    "            # if data.label == 2:\n",
    "            data.label = torch.tensor(1)\n",
    "        dl_filterd.append(data)\n",
    "\n",
    "    len(dl_filterd)\n",
    "    return dl_filterd\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_54096\\598882657.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/moments_pearson/')\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLevelConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_filterd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempted to use an uninitialized parameter in <method 'numel' of 'torch._C._TensorBase' objects>. This error happens when you are using a `LazyModule` or explicitly manipulating `torch.nn.parameter.UninitializedParameter` objects. When using LazyModules Call `forward` with a dummy batch to initialize the parameters before calling torch functions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mp\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mdouble()\n",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mdouble()\n",
      "File \u001b[1;32mc:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\parameter.py:156\u001b[0m, in \u001b[0;36mUninitializedTensorMixin.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m__torch_function__(func, types, args, kwargs)\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempted to use an uninitialized parameter in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis error happens when you are using a `LazyModule` or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplicitly manipulating `torch.nn.parameter.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjects. When using LazyModules Call `forward` with a dummy batch \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto initialize the parameters before calling torch functions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Attempted to use an uninitialized parameter in <method 'numel' of 'torch._C._TensorBase' objects>. This error happens when you are using a `LazyModule` or explicitly manipulating `torch.nn.parameter.UninitializedParameter` objects. When using LazyModules Call `forward` with a dummy batch to initialize the parameters before calling torch functions"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
