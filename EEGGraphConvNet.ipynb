{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEGGraphConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, BatchNorm, global_add_pool, ChebConv, global_max_pool, SAGPooling, GATConv, GATv2Conv, TransformerConv, SuperGATConv, global_mean_pool, Linear\n",
    "from torch.nn import BatchNorm1d\n",
    "from math import floor\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGGraphConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduced_sensors=True, sfreq=None, batch_size=64):\n",
    "        super(EEGGraphConvNet, self).__init__()\n",
    "        # Define and initialize hyperparameters\n",
    "        self.sfreq = sfreq\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = 6 \n",
    "        \n",
    "        # Layers definition\n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(self.input_size, 16, cached=True, normalize=False)\n",
    "        self.conv2 = GCNConv(16, 32, cached=True, normalize=False)\n",
    "        self.conv3 = GCNConv(32, 64, cached=True, normalize=False)\n",
    "        self.conv4 = GCNConv(64, 50, cached=True, normalize=False)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(50, 30)\n",
    "        self.fc2 = nn.Linear(30, 20)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "        # Xavier initializacion for fully connected layers\n",
    "        self.fc1.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc2.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc3.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weigth, batch):\n",
    "        # Perform all graph convolutions\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index, edge_weigth), negative_slope=0.01)\n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = F.leaky_relu(self.conv2(x, edge_index, edge_weigth), negative_slope=0.01)\n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = F.leaky_relu(self.conv3(x, edge_index, edge_weigth), negative_slope=0.01)\n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        conv_out = self.conv4(x, edge_index, edge_weigth)\n",
    "        # Perform batch normalization\n",
    "        batch_norm_out = F.leaky_relu(self.batch_norm(conv_out), negative_slope=0.01)\n",
    "        #x = F.dropout(batch_norm_out, p=0.2, training=self.training)\n",
    "        \n",
    "        # Global add pooling\n",
    "        mean_pool = global_add_pool(batch_norm_out, batch=batch)\n",
    "        \n",
    "        # Apply fully connected layters\n",
    "        out = F.leaky_relu(self.fc1(mean_pool), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        \n",
    "        out = F.leaky_relu(self.fc2(out), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        \n",
    "        out = F.leaky_relu(self.fc3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corrected_data_list(path):\n",
    "    data_list = list()\n",
    "    for file in path.iterdir():\n",
    "        data_list.append(torch.load(file))\n",
    "    corrected_data_list = list()\n",
    "    for data in data_list:\n",
    "    # print(data)\n",
    "        data = torch_geometric.data.Data(\n",
    "            x=torch.tensor(data.x),\n",
    "            edge_index=torch.tensor(data.edge_index),\n",
    "            edge_attr=torch.tensor(data.edge_attr),\n",
    "            label=torch.tensor(data.label),\n",
    "        )\n",
    "        corrected_data_list.append(data)\n",
    "       \n",
    "       \n",
    "    rm = [\n",
    "      7,\n",
    "      14+1,\n",
    "      14+2,\n",
    "      17+3,\n",
    "      17+4,\n",
    "      26+5,\n",
    "      38+6,\n",
    "      54+7,\n",
    "      65+8,\n",
    "      69+9\n",
    "      ]\n",
    "\n",
    "    dl = list()\n",
    "    start = 0\n",
    "    for r in rm:\n",
    "        dl.extend(corrected_data_list[start:r])\n",
    "        start = r + 1\n",
    "\n",
    "    dl.extend(corrected_data_list[start:])\n",
    "    dl_filterd = list()\n",
    "    for data in dl:\n",
    "        if data.label == 2:\n",
    "            # print(data.label)\n",
    "            # if data.label == 2:\n",
    "            data.label = torch.tensor(1)\n",
    "        dl_filterd.append(data)\n",
    "\n",
    "    len(dl_filterd)\n",
    "    return dl_filterd\n",
    "\n",
    "def print_classification_report(y_pred, y_true):\n",
    "    print(classification_report(y_true, y_pred))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge :: Pearson Node:: Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch_geometric\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_25072\\114600516.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/moments_pearson/')\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EEGGraphConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 8310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGGraphConvNet(\n",
       "  (conv1): GCNConv(6, 16)\n",
       "  (conv2): GCNConv(16, 32)\n",
       "  (conv3): GCNConv(32, 64)\n",
       "  (conv4): GCNConv(64, 50)\n",
       "  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=50, out_features=30, bias=True)\n",
       "  (fc2): Linear(in_features=30, out_features=20, bias=True)\n",
       "  (fc3): Linear(in_features=20, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=4.153304\t acc=0.653846\n",
      "Test  : epoch=1\t loss=9.012556\t acc=0.437500\n",
      "\n",
      "Train : epoch=2\t loss=3.827767\t acc=0.653846\n",
      "Test  : epoch=2\t loss=27.654194\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=3.589363\t acc=0.653846\n",
      "Test  : epoch=3\t loss=35.080489\t acc=0.500000\n",
      "\n",
      "Train : epoch=4\t loss=3.695025\t acc=0.653846\n",
      "Test  : epoch=4\t loss=31.443337\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=3.595305\t acc=0.653846\n",
      "Test  : epoch=5\t loss=46.184410\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=3.604514\t acc=0.653846\n",
      "Test  : epoch=6\t loss=77.153910\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=3.677555\t acc=0.653846\n",
      "Test  : epoch=7\t loss=66.017878\t acc=0.375000\n",
      "\n",
      "Train : epoch=8\t loss=3.503490\t acc=0.653846\n",
      "Test  : epoch=8\t loss=52.486600\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=3.487125\t acc=0.653846\n",
      "Test  : epoch=9\t loss=34.914284\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=3.659228\t acc=0.653846\n",
      "Test  : epoch=10\t loss=43.592380\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print_classification_report(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moments PLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_25072\\114600516.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/moments_pli/')\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 8310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGGraphConvNet(\n",
       "  (conv1): GCNConv(6, 16)\n",
       "  (conv2): GCNConv(16, 32)\n",
       "  (conv3): GCNConv(32, 64)\n",
       "  (conv4): GCNConv(64, 50)\n",
       "  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=50, out_features=30, bias=True)\n",
       "  (fc2): Linear(in_features=30, out_features=20, bias=True)\n",
       "  (fc3): Linear(in_features=20, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EEGGraphConvNet()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=2.368049\t acc=0.628205\n",
      "Test  : epoch=1\t loss=994697.199657\t acc=0.500000\n",
      "\n",
      "Train : epoch=2\t loss=3.841162\t acc=0.653846\n",
      "Test  : epoch=2\t loss=1278121.169192\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=4.044218\t acc=0.653846\n",
      "Test  : epoch=3\t loss=2784205.288110\t acc=0.500000\n",
      "\n",
      "Train : epoch=4\t loss=5.505245\t acc=0.653846\n",
      "Test  : epoch=4\t loss=1098611.138698\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=4.126309\t acc=0.653846\n",
      "Test  : epoch=5\t loss=2034908.642611\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=4.303449\t acc=0.653846\n",
      "Test  : epoch=6\t loss=1928618.711670\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=4.044598\t acc=0.653846\n",
      "Test  : epoch=7\t loss=1575040.023419\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=3.881790\t acc=0.653846\n",
      "Test  : epoch=8\t loss=1611090.943063\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=3.836806\t acc=0.653846\n",
      "Test  : epoch=9\t loss=1490387.901666\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=3.834923\t acc=0.653846\n",
      "Test  : epoch=10\t loss=1504149.384253\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSD CSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_25072\\114600516.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/psd_csd/')\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EEGGraphConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 8310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGGraphConvNet(\n",
       "  (conv1): GCNConv(6, 16)\n",
       "  (conv2): GCNConv(16, 32)\n",
       "  (conv3): GCNConv(32, 64)\n",
       "  (conv4): GCNConv(64, 50)\n",
       "  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=50, out_features=30, bias=True)\n",
       "  (fc2): Linear(in_features=30, out_features=20, bias=True)\n",
       "  (fc3): Linear(in_features=20, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=0.738278\t acc=0.653846\n",
      "Test  : epoch=1\t loss=0.706056\t acc=0.500000\n",
      "\n",
      "Train : epoch=2\t loss=0.928931\t acc=0.653846\n",
      "Test  : epoch=2\t loss=0.916596\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=1.595207\t acc=0.653846\n",
      "Test  : epoch=3\t loss=1.592788\t acc=0.500000\n",
      "\n",
      "Train : epoch=4\t loss=3.031802\t acc=0.653846\n",
      "Test  : epoch=4\t loss=3.482221\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=3.326565\t acc=0.653846\n",
      "Test  : epoch=5\t loss=3.629290\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=3.314263\t acc=0.653846\n",
      "Test  : epoch=6\t loss=3.632210\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=3.307780\t acc=0.653846\n",
      "Test  : epoch=7\t loss=3.642444\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=3.303494\t acc=0.653846\n",
      "Test  : epoch=8\t loss=3.646749\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=3.299511\t acc=0.653846\n",
      "Test  : epoch=9\t loss=3.652948\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=3.295911\t acc=0.653846\n",
      "Test  : epoch=10\t loss=3.658892\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print_classification_report(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSD Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_25072\\114600516.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/psd_pearson/')\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EEGGraphConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 8310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGGraphConvNet(\n",
       "  (conv1): GCNConv(6, 16)\n",
       "  (conv2): GCNConv(16, 32)\n",
       "  (conv3): GCNConv(32, 64)\n",
       "  (conv4): GCNConv(64, 50)\n",
       "  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=50, out_features=30, bias=True)\n",
       "  (fc2): Linear(in_features=30, out_features=20, bias=True)\n",
       "  (fc3): Linear(in_features=20, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=4.008557\t acc=0.512821\n",
      "Test  : epoch=1\t loss=0.670468\t acc=0.562500\n",
      "\n",
      "Train : epoch=2\t loss=2.933610\t acc=0.653846\n",
      "Test  : epoch=2\t loss=4.252371\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=4.000153\t acc=0.653846\n",
      "Test  : epoch=3\t loss=4.602239\t acc=0.500000\n",
      "\n",
      "Train : epoch=4\t loss=4.089587\t acc=0.653846\n",
      "Test  : epoch=4\t loss=7.326805\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=4.263249\t acc=0.653846\n",
      "Test  : epoch=5\t loss=10.544871\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=4.432763\t acc=0.653846\n",
      "Test  : epoch=6\t loss=14.153053\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=4.664533\t acc=0.653846\n",
      "Test  : epoch=7\t loss=17.380319\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=2.842566\t acc=0.653846\n",
      "Test  : epoch=8\t loss=16.172649\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=2.828105\t acc=0.653846\n",
      "Test  : epoch=9\t loss=16.015167\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=2.818375\t acc=0.653846\n",
      "Test  : epoch=10\t loss=16.230353\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print_classification_report(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
