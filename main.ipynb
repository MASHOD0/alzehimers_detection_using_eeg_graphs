{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, BatchNorm, global_add_pool, ChebConv, global_max_pool, SAGPooling, GATConv, GATv2Conv, TransformerConv, SuperGATConv, global_mean_pool, Linear\n",
    "from torch.nn import BatchNorm1d\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGGraphConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduced_sensors=True, sfreq=None, batch_size=256):\n",
    "        super(EEGGraphConvNet, self).__init__()\n",
    "        # Define and initialize hyperparameters\n",
    "        self.sfreq = sfreq\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = 8 if reduced_sensors else 62\n",
    "        \n",
    "        # Layers definition\n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(-1, 16, cached=True, normalize=False)\n",
    "        self.conv2 = GCNConv(16, 32, cached=True, normalize=False)\n",
    "        self.conv3 = GCNConv(32, 64, cached=True, normalize=False)\n",
    "        self.conv4 = GCNConv(64, 50, cached=True, normalize=False)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(50, 30)\n",
    "        self.fc2 = nn.Linear(30, 20)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "        # Xavier initializacion for fully connected layers\n",
    "        self.fc1.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc2.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc3.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weigth, batch):\n",
    "        # Perform all graph convolutions\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index, edge_weigth), negative_slope=0.01)\n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = F.leaky_relu(self.conv2(x, edge_index, edge_weigth), negative_slope=0.01)\n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = F.leaky_relu(self.conv3(x, edge_index, edge_weigth), negative_slope=0.01)\n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        conv_out = self.conv4(x, edge_index, edge_weigth)\n",
    "        # Perform batch normalization\n",
    "        batch_norm_out = F.leaky_relu(self.batch_norm(conv_out), negative_slope=0.01)\n",
    "        #x = F.dropout(batch_norm_out, p=0.2, training=self.training)\n",
    "        \n",
    "        # Global add pooling\n",
    "        mean_pool = global_add_pool(batch_norm_out, batch=batch)\n",
    "        \n",
    "        # Apply fully connected layters\n",
    "        out = F.leaky_relu(self.fc1(mean_pool), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        \n",
    "        out = F.leaky_relu(self.fc2(out), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        \n",
    "        out = F.leaky_relu(self.fc3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGGraphConvNetTemporal(nn.Module):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(EEGGraphConvNetTemporal, self).__init__()\n",
    "        # Layers definition\n",
    "        input_size = kwargs.pop(\"input_size\", 1280)        \n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(input_size, 640, cached=True, normalize=False)\n",
    "        self.conv2 = GCNConv(640, 512, cached=True, normalize=False)\n",
    "        self.conv3 = GCNConv(512, 256, cached=True, normalize=False)\n",
    "        self.conv4 = GCNConv(256, 256, cached=True, normalize=False)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm1 = BatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm2 = BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm3 = BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm4 = BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        \n",
    "        # Xavier initializacion for fully connected layers\n",
    "        self.fc1.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc2.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc3.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weigth, batch):\n",
    "        x = F.leaky_relu(self.batch_norm1(self.conv1(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.batch_norm2(self.conv2(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.batch_norm3(self.conv3(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.batch_norm4(self.conv4(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        #out = F.dropout(x, p = 0.2, training=self.training)\n",
    "\n",
    "        add_pool = global_add_pool(x, batch=batch)\n",
    "        out = F.leaky_relu(self.fc1(add_pool), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        out = F.leaky_relu(self.fc2(out), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        out = F.leaky_relu(self.fc3(out))\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGGraphConvNetLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduced_sensors=True, sfreq=None, batch_size=256, **kwargs):\n",
    "        super(EEGGraphConvNetLSTM, self).__init__()\n",
    "        # Define and initialize hyperparameters\n",
    "        self.sfreq = sfreq\n",
    "        self.batch_size = batch_size\n",
    "        #self.input_size = 8 if reduced_sensors else 62\n",
    "        \n",
    "        # Layers definition\n",
    "        input_size = kwargs.pop(\"input_size\", 1280)\n",
    "        hidden_dim = 320\n",
    "        \n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(1280, 640, cached=True, normalize=False)\n",
    "        self.conv2 = GCNConv(640, 512, cached=True, normalize=False)\n",
    "        self.conv3 = GCNConv(512, 256, cached=True, normalize=False)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm1 = BatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm2 = BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm3 = BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm4 = BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.lstm = nn.LSTM(256, 256, 1, dropout=0.2)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        \n",
    "        # Xavier initializacion for fully connected layers\n",
    "        self.fc1.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc2.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc3.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weigth, batch):\n",
    "        \n",
    "        x = F.leaky_relu(self.batch_norm1(self.conv1(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.batch_norm2(self.conv2(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.batch_norm3(self.conv3(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x, _ = self.lstm(x)\n",
    "        #x = F.leaky_relu(self.batch_norm4(self.conv4(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        #x = F.dropout(batch_norm_out, p=0.2, training=self.training)\n",
    "        \n",
    "        # Global add pooling\n",
    "        add_pool = global_add_pool(x, batch=batch)\n",
    "        # Apply fully connected layters\n",
    "        out = F.leaky_relu(self.fc1(add_pool), negative_slope=0.01)\n",
    "        out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        out = F.leaky_relu(self.fc2(out), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        out = F.leaky_relu(self.fc3(out))\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGConvNetMini(nn.Module):\n",
    "    \"\"\"Same as EEGGraphConvNet but with fewer \n",
    "    convolutional layers\n",
    "    \"\"\"\n",
    "    def __init__(self, reduced_sensors=True, sfreq=None, batch_size=256):\n",
    "        super(EEGConvNetMini, self).__init__()\n",
    "        # Define and initialize hyperparameters\n",
    "        self.sfreq = sfreq\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = 8 if reduced_sensors else 62\n",
    "        \n",
    "        # Layers definition\n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(-1, 16, cached=True, normalize=False)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(16, 8)\n",
    "        self.fc2 = nn.Linear(8, 4)\n",
    "        self.fc3 = nn.Linear(4, 2)\n",
    "        \n",
    "        # Xavier initializacion for fully connected layers\n",
    "        self.fc1.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc2.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc3.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weigth, batch):\n",
    "        # Perform all graph convolutions\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index, edge_weigth), negative_slope=0.01)\n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Perform batch normalization\n",
    "        x = F.leaky_relu(self.batch_norm(x), negative_slope=0.01)\n",
    "        #x = F.dropout(batch_norm_out, p=0.2, training=self.training)\n",
    "        \n",
    "        # Global add pooling\n",
    "        mean_pool = global_add_pool(x, batch=batch)\n",
    "        \n",
    "        # Apply fully connected layters\n",
    "        out = F.leaky_relu(self.fc1(mean_pool), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        \n",
    "        out = F.leaky_relu(self.fc2(out), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        \n",
    "        out = F.leaky_relu(self.fc3(out))\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGConvNetMiniV2(nn.Module):\n",
    "    \"\"\"Same as EEGGraphConvNet but with fewer \n",
    "    convolutional layers\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(EEGConvNetMiniV2, self).__init__()\n",
    "        # Layers definition\n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(-1, 32, cached=True, normalize=False) #16\n",
    "        self.conv2 = GCNConv(32, 64, cached=True, normalize=False) #32\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm1 = BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm2 = BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 2)\n",
    "        \n",
    "        # Xavier initializacion for fully connected layers\n",
    "        self.fc1.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc2.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc3.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weigth, batch):\n",
    "        # Perform all graph convolutions\n",
    "        x = F.leaky_relu(self.batch_norm1(self.conv1(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.batch_norm2(self.conv2(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        #x = F.dropout(batch_norm_out, p=0.2, training=self.training)\n",
    "        # Global add pooling\n",
    "        mean_pool = global_add_pool(x, batch=batch)\n",
    "        \n",
    "        # Apply fully connected layters\n",
    "        out = F.leaky_relu(self.fc1(mean_pool), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        \n",
    "        out = F.leaky_relu(self.fc2(out), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        \n",
    "        out = F.leaky_relu(self.fc3(out))\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGConvNetMiniV3(nn.Module):\n",
    "    \"\"\"Same as EEGGraphConvNet but with fewer \n",
    "    convolutional layers\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(EEGConvNetMiniV3, self).__init__()\n",
    "        # Layers definition\n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(-1, 32, cached=True, normalize=False)\n",
    "        self.conv2 = GCNConv(32, 64, cached=True, normalize=False)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm1 = BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm2 = BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 2)\n",
    "        \n",
    "        # Xavier initializacion for fully connected layers\n",
    "        self.fc1.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc2.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc3.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weigth, batch):\n",
    "        x = F.leaky_relu(self.batch_norm1(self.conv1(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.batch_norm2(self.conv2(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        mean_pool = global_add_pool(x, batch=batch)\n",
    "        out = F.leaky_relu(self.fc1(mean_pool), negative_slope=0.01)        \n",
    "        out = F.leaky_relu(self.fc2(out), negative_slope=0.01)\n",
    "        out = F.leaky_relu(self.fc3(out))\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGConvNetMiniV2Attention(nn.Module):\n",
    "    \"\"\"Same as EEGGraphConvNet but with fewer \n",
    "    convolutional layers\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(EEGConvNetMiniV2Attention, self).__init__()\n",
    "        # Layers definition\n",
    "        # Graph convolutional layers\n",
    "        conv_layers = {\n",
    "            \"gatconv\": GATConv,\n",
    "            \"gatconv2\": GATv2Conv,\n",
    "            \"transformer\": TransformerConv,\n",
    "            \"super\": SuperGATConv\n",
    "        }\n",
    "        \n",
    "        AttentionConv = conv_layers[kwargs.get(\"attention_conv\", \"gatconv\")]\n",
    "        nheads = kwargs.get(\"nheads\", 1)\n",
    "        hidden_dim = kwargs.get(\"hidden_dim\", 32)\n",
    "        output_dim = kwargs.get(\"output_dim\", 64)\n",
    "        concat = kwargs.get(\"concat\", True)\n",
    "        nclasses = kwargs.get(\"nclasses\", 2)\n",
    "        \n",
    "        self.conv1 = AttentionConv(-1, hidden_dim, heads=nheads, concat=concat) #16\n",
    "        self.conv2 = AttentionConv(hidden_dim * nheads, output_dim, heads=nheads, concat=concat)\n",
    "        self.conv3 = AttentionConv(output_dim * nheads, output_dim * nheads * 2, heads=nheads, concat=concat)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm1 = BatchNorm(hidden_dim * nheads, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm2 = BatchNorm(output_dim * nheads, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm3 = BatchNorm(output_dim * nheads * 2 * nheads, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) #output_dim * nheads * 2 * 2\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(output_dim * nheads * 2 * nheads, output_dim * nheads * 2)\n",
    "        self.fc2 = nn.Linear(output_dim * nheads * 2, output_dim * nheads)\n",
    "        self.fc3 = nn.Linear((output_dim * nheads), nclasses)\n",
    "        \n",
    "        # Xavier initializacion for fully connected layers\n",
    "        self.fc1.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc2.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc3.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weigth, batch):\n",
    "        # Perform all graph convolutions\n",
    "        x = F.leaky_relu(self.batch_norm1(self.conv1(x, edge_index)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.batch_norm2(self.conv2(x, edge_index)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.batch_norm3(self.conv3(x, edge_index)), negative_slope=0.01)\n",
    "\n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        #x = F.dropout(batch_norm_out, p=0.2, training=self.training)\n",
    "        \n",
    "        # Global add pooling\n",
    "        mean_pool = global_add_pool(x, batch=batch)\n",
    "        #print(mean_pool.size())\n",
    "        \n",
    "        # Apply fully connected layters\n",
    "        out = F.leaky_relu(self.fc1(mean_pool), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        \n",
    "        out = F.leaky_relu(self.fc2(out), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        \n",
    "        out = F.leaky_relu(self.fc3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLevelConvNet(nn.Module):\n",
    "    \"\"\"Same as EEGGraphConvNet but with fewer \n",
    "    convolutional layers\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MultiLevelConvNet, self).__init__()\n",
    "        # Layers definition\n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(-1, 32, cached=True, normalize=False)\n",
    "        self.conv2 = GCNConv(32, 32, cached=True, normalize=False)\n",
    "        self.conv3 = GCNConv(32, 64, cached=True, normalize=False)\n",
    "        \n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm1 = BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm2 = BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm3 = BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(32, 64)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(192, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "        \n",
    "        # Xavier initializacion for fully connected layers\n",
    "        self.fc1.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc2.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc3.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weigth, batch):\n",
    "        x1 = F.leaky_relu(self.batch_norm1(self.conv1(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x2 = F.leaky_relu(self.batch_norm2(self.conv2(x1, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x3 = F.leaky_relu(self.batch_norm3(self.conv3(x2, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        \n",
    "        add_pool1 = global_add_pool(x1, batch=batch)\n",
    "        add_pool2 = global_add_pool(x2, batch=batch)\n",
    "        add_pool3 = global_add_pool(x3, batch=batch)\n",
    "        \n",
    "        out1 = F.leaky_relu(self.fc1(add_pool1), negative_slope=0.01)        \n",
    "        out2 = F.leaky_relu(self.fc2(add_pool2), negative_slope=0.01)        \n",
    "        out3 = F.leaky_relu(self.fc3(add_pool3), negative_slope=0.01)\n",
    "        \n",
    "        out = torch.cat((out1, out2, out3), dim=1)        \n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAGE(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MAGE, self).__init__()\n",
    "        # Layers definition\n",
    "        # Graph convolutional layers\n",
    "        conv_layers = {\n",
    "            \"gatconv\": GATConv,\n",
    "            \"gatconv2\": GATv2Conv,\n",
    "            \"transformer\": TransformerConv,\n",
    "            \"super\": SuperGATConv\n",
    "        }\n",
    "        \n",
    "        AttentionConv = conv_layers[kwargs.get(\"attention_conv\", \"gatconv\")]\n",
    "        nheads = kwargs.get(\"nheads\", 1)\n",
    "        hidden_dim = kwargs.get(\"hidden_dim\", 32)\n",
    "        output_dim = kwargs.get(\"output_dim\", 64)\n",
    "        concat = kwargs.get(\"concat\", True)\n",
    "        \n",
    "        nclasses = kwargs.get(\"nclasses\", 2)\n",
    "        \n",
    "        self.conv1 = AttentionConv(-1, hidden_dim, heads=nheads, concat=concat) #16\n",
    "        self.conv2 = AttentionConv(hidden_dim * nheads, hidden_dim * nheads, heads=nheads, concat=concat)\n",
    "        self.conv3 = AttentionConv(hidden_dim * nheads * nheads, hidden_dim * nheads * nheads, heads=nheads, concat=concat)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm1 = BatchNorm(hidden_dim * nheads, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm2 = BatchNorm(hidden_dim * nheads * nheads, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm3 = BatchNorm(hidden_dim * nheads * nheads * nheads, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_dim * nheads, hidden_dim * nheads)\n",
    "        self.fc2 = nn.Linear(output_dim * nheads, output_dim * nheads)\n",
    "        self.fc3 = nn.Linear(output_dim * nheads , output_dim * nheads * nheads)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear((hidden_dim * nheads) + (hidden_dim * nheads * nheads) + (hidden_dim * nheads * nheads * nheads), 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 2),\n",
    "        )\n",
    "        \n",
    "        # Xavier initializacion for fully connected layers\n",
    "        self.fc1.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc2.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc3.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weigth, batch):\n",
    "        # Perform all graph convolutions\n",
    "        x1 = F.leaky_relu(self.batch_norm1(self.conv1(x, edge_index)), negative_slope=0.01)\n",
    "        x2 = F.leaky_relu(self.batch_norm2(self.conv2(x1, edge_index)), negative_slope=0.01)\n",
    "        x3 = F.leaky_relu(self.batch_norm3(self.conv3(x2, edge_index)), negative_slope=0.01)\n",
    "        \n",
    "        # Global add pooling\n",
    "        add_pool1 = global_add_pool(x1, batch=batch)\n",
    "        add_pool2 = global_add_pool(x2, batch=batch)\n",
    "        add_pool3 = global_add_pool(x3, batch=batch)\n",
    "        \n",
    "        #out1 = F.leaky_relu(self.fc1(add_pool1), negative_slope=0.01)        \n",
    "        #out2 = F.leaky_relu(self.fc2(add_pool2), negative_slope=0.01)        \n",
    "        #out3 = F.leaky_relu(self.fc3(add_pool3), negative_slope=0.01)\n",
    "\n",
    "        out = torch.cat((add_pool1, add_pool2, add_pool3), dim=1)        \n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score\n",
    "from numpy import mean\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "monements_pearson_path = Path('graphs/moments_pearson/')\n",
    "for file in os.listdir(monements_pearson_path):\n",
    "    data = torch.load(monements_pearson_path.joinpath(file))\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list, test_list = train_test_split(data_list, test_size=0.3, random_state=4744)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=2),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=0),\n",
       " Data(x=[19, 6], edge_index=[2, 361], edge_attr=[19, 19], label=1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_list, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_list, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EEGConvNetMiniV2Attention(attention_conv=\"transformer\", nheads=2, hidden_dim=16, output_dim=32, nclasses=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.76153398,  0.47364111,  0.49625678,  0.57560999,\n",
       "         0.63875954,  0.5465498 ,  0.3406173 ,  0.28532149,  0.24783629,\n",
       "         0.41335199,  0.68326831,  0.50928091,  0.59655049,  0.3751361 ,\n",
       "         0.35948346,  0.46662683,  0.5233104 ,  0.23928294],\n",
       "       [ 0.76153398,  0.        ,  0.43991159,  0.72888121,  0.39031691,\n",
       "         0.52095233,  0.3662653 ,  0.12544106,  0.10757305,  0.16773811,\n",
       "         0.35530036,  0.6332677 ,  0.40574011,  0.51353854,  0.26981602,\n",
       "         0.30603446,  0.41618273,  0.31112191,  0.37064108],\n",
       "       [ 0.47364111,  0.43991159,  1.        ,  0.81268732,  0.05537708,\n",
       "         0.366651  ,  0.28468176,  0.20794254,  0.35429072,  0.56324822,\n",
       "         0.93098389,  0.51106258,  0.82676685,  0.38853327,  0.81152884,\n",
       "         0.80536848,  0.8259073 , -0.00554757,  0.57646427],\n",
       "       [ 0.49625678,  0.72888121,  0.81268732,  1.        ,  0.05352186,\n",
       "         0.31294323,  0.18454596,  0.10057525,  0.20591791,  0.40854138,\n",
       "         0.72625062,  0.56639043,  0.60216542,  0.34160977,  0.57369925,\n",
       "         0.58922895,  0.62543219, -0.02242662,  0.51329521],\n",
       "       [ 0.57560999,  0.39031691,  0.05537708,  0.05352186,  0.        ,\n",
       "         0.78231524,  0.79345252,  0.61455097,  0.4658413 ,  0.24408306,\n",
       "        -0.00803617,  0.3447867 ,  0.28362489,  0.64938225,  0.17818755,\n",
       "         0.15246927,  0.21296634,  0.81740107,  0.24360475],\n",
       "       [ 0.63875954,  0.52095233,  0.366651  ,  0.31294323,  0.78231524,\n",
       "         1.        ,  0.87019992,  0.56943911,  0.56283333,  0.43370277,\n",
       "         0.35052705,  0.61818104,  0.63207244,  0.80822156,  0.50128719,\n",
       "         0.54027665,  0.5332969 ,  0.79014188,  0.48990517],\n",
       "       [ 0.5465498 ,  0.3662653 ,  0.28468176,  0.18454596,  0.79345252,\n",
       "         0.87019992,  1.        ,  0.66752515,  0.66867876,  0.51948277,\n",
       "         0.25023257,  0.47808309,  0.53736435,  0.77109289,  0.49554972,\n",
       "         0.45836175,  0.41921906,  0.79394961,  0.41460649],\n",
       "       [ 0.3406173 ,  0.12544106,  0.20794254,  0.10057525,  0.61455097,\n",
       "         0.56943911,  0.66752515,  0.        ,  0.68719748,  0.66884245,\n",
       "         0.13630622,  0.26121365,  0.36881057,  0.39424153,  0.50494479,\n",
       "         0.36612191,  0.39254913,  0.61742883,  0.42254613],\n",
       "       [ 0.28532149,  0.10757305,  0.35429072,  0.20591791,  0.4658413 ,\n",
       "         0.56283333,  0.66867876,  0.68719748,  1.        ,  0.82588944,\n",
       "         0.35638336,  0.13653477,  0.4811363 ,  0.51644965,  0.62911802,\n",
       "         0.4886568 ,  0.30791849,  0.44123872,  0.3761449 ],\n",
       "       [ 0.24783629,  0.16773811,  0.56324822,  0.40854138,  0.24408306,\n",
       "         0.43370277,  0.51948277,  0.66884245,  0.82588944,  0.        ,\n",
       "         0.54305595,  0.20848549,  0.57801029,  0.41594912,  0.7828486 ,\n",
       "         0.65188927,  0.4899838 ,  0.1681018 ,  0.53136863],\n",
       "       [ 0.41335199,  0.35530036,  0.93098389,  0.72625062, -0.00803617,\n",
       "         0.35052705,  0.25023257,  0.13630622,  0.35638336,  0.54305595,\n",
       "         1.        ,  0.44752494,  0.87267903,  0.38943063,  0.79261653,\n",
       "         0.79246362,  0.792489  , -0.07055455,  0.53368784],\n",
       "       [ 0.68326831,  0.6332677 ,  0.51106258,  0.56639043,  0.3447867 ,\n",
       "         0.61818104,  0.47808309,  0.26121365,  0.13653477,  0.20848549,\n",
       "         0.44752494,  0.        ,  0.57318071,  0.57805731,  0.46097147,\n",
       "         0.53964026,  0.55059033,  0.44594615,  0.37883245],\n",
       "       [ 0.50928091,  0.40574011,  0.82676685,  0.60216542,  0.28362489,\n",
       "         0.63207244,  0.53736435,  0.36881057,  0.4811363 ,  0.57801029,\n",
       "         0.87267903,  0.57318071,  0.        ,  0.56619218,  0.82825705,\n",
       "         0.8669248 ,  0.87129251,  0.26899203,  0.67011136],\n",
       "       [ 0.59655049,  0.51353854,  0.38853327,  0.34160977,  0.64938225,\n",
       "         0.80822156,  0.77109289,  0.39424153,  0.51644965,  0.41594912,\n",
       "         0.38943063,  0.57805731,  0.56619218,  0.        ,  0.43628046,\n",
       "         0.47342373,  0.36773862,  0.54259546,  0.39017332],\n",
       "       [ 0.3751361 ,  0.26981602,  0.81152884,  0.57369925,  0.17818755,\n",
       "         0.50128719,  0.49554972,  0.50494479,  0.62911802,  0.7828486 ,\n",
       "         0.79261653,  0.46097147,  0.82825705,  0.43628046,  1.        ,\n",
       "         0.91148125,  0.83055973,  0.22197965,  0.74164671],\n",
       "       [ 0.35948346,  0.30603446,  0.80536848,  0.58922895,  0.15246927,\n",
       "         0.54027665,  0.45836175,  0.36612191,  0.4886568 ,  0.65188927,\n",
       "         0.79246362,  0.53964026,  0.8669248 ,  0.47342373,  0.91148125,\n",
       "         0.        ,  0.83615309,  0.18946556,  0.75271637],\n",
       "       [ 0.46662683,  0.41618273,  0.8259073 ,  0.62543219,  0.21296634,\n",
       "         0.5332969 ,  0.41921906,  0.39254913,  0.30791849,  0.4899838 ,\n",
       "         0.792489  ,  0.55059033,  0.87129251,  0.36773862,  0.83055973,\n",
       "         0.83615309,  0.        ,  0.28078392,  0.78227754],\n",
       "       [ 0.5233104 ,  0.31112191, -0.00554757, -0.02242662,  0.81740107,\n",
       "         0.79014188,  0.79394961,  0.61742883,  0.44123872,  0.1681018 ,\n",
       "        -0.07055455,  0.44594615,  0.26899203,  0.54259546,  0.22197965,\n",
       "         0.18946556,  0.28078392,  1.        ,  0.25603465],\n",
       "       [ 0.23928294,  0.37064108,  0.57646427,  0.51329521,  0.24360475,\n",
       "         0.48990517,  0.41460649,  0.42254613,  0.3761449 ,  0.53136863,\n",
       "         0.53368784,  0.37883245,  0.67011136,  0.39017332,  0.74164671,\n",
       "         0.75271637,  0.78227754,  0.25603465,  0.        ]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list[0].edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19, 1])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [data.label for data in train_list]\n",
    "y_test = [data.label for data in test_list]\n",
    "\n",
    "x_train = [[np.std(x) for x in data.edge_attr] for data in train_list]\n",
    "x_test = [[np.std(x) for x in data.edge_attr] for data in test_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.20014946274504558,\n",
       "  0.12447249222945671,\n",
       "  0.20206889417789395,\n",
       "  0.10687509460745652,\n",
       "  0.19429647331120448,\n",
       "  0.15215177982627745,\n",
       "  0.19300119925611323,\n",
       "  0.091698260154082,\n",
       "  0.19429114864575286,\n",
       "  0.18561679429172143,\n",
       "  0.1913816648136844,\n",
       "  0.09775919874072861,\n",
       "  0.12638639185120829,\n",
       "  0.1944982151156955,\n",
       "  0.09504071392195745,\n",
       "  0.18936645029355975,\n",
       "  0.21222153774759922,\n",
       "  0.12046977651957445,\n",
       "  0.19349146186275176],\n",
       " [0.21323692587623966,\n",
       "  0.18275695353908863,\n",
       "  0.22120212560049085,\n",
       "  0.052151442105203505,\n",
       "  0.22087262435985364,\n",
       "  0.22103533490635666,\n",
       "  0.054777814783846454,\n",
       "  0.22206304483894965,\n",
       "  0.21939344621051346,\n",
       "  0.2089790476064607,\n",
       "  0.2193549368985272,\n",
       "  0.21715985454465808,\n",
       "  0.221547393588728,\n",
       "  0.22199551076942914,\n",
       "  0.2222819489158725,\n",
       "  0.21832652369948236,\n",
       "  0.22111853656641395,\n",
       "  0.22222618206463238,\n",
       "  0.05498192738373229],\n",
       " [0.1761144976688873,\n",
       "  0.18367847571793727,\n",
       "  0.19799314464374299,\n",
       "  0.2110707529098685,\n",
       "  0.1421918288851274,\n",
       "  0.15998257488810183,\n",
       "  0.22340800323973717,\n",
       "  0.15831648346823216,\n",
       "  0.10777198106075211,\n",
       "  0.16059638136324195,\n",
       "  0.1997466725434706,\n",
       "  0.16185868080679314,\n",
       "  0.21923510002032962,\n",
       "  0.21636631298537629,\n",
       "  0.1729859605304071,\n",
       "  0.18477788298534736,\n",
       "  0.14183185215435454,\n",
       "  0.21188302973534925,\n",
       "  0.19152868114050128],\n",
       " [0.20662076965429738,\n",
       "  0.2746608244553515,\n",
       "  0.2835142802551684,\n",
       "  0.2687481693315485,\n",
       "  0.26469589076278227,\n",
       "  0.2258792008104027,\n",
       "  0.2279700738156385,\n",
       "  0.19159186939136466,\n",
       "  0.28295366011118633,\n",
       "  0.22277898160226992,\n",
       "  0.2310743691628926,\n",
       "  0.21074049443918372,\n",
       "  0.2770524953476657,\n",
       "  0.11506414970830516,\n",
       "  0.2637394966434899,\n",
       "  0.13954305575088205,\n",
       "  0.25460898086869527,\n",
       "  0.09532274005753938,\n",
       "  0.28188534539229537],\n",
       " [0.23336593442146947,\n",
       "  0.24025550539854823,\n",
       "  0.2709737103044229,\n",
       "  0.23656387147585384,\n",
       "  0.21300330000088735,\n",
       "  0.2691286088065858,\n",
       "  0.1898904172188134,\n",
       "  0.24794917403987424,\n",
       "  0.1289286596645397,\n",
       "  0.2383269100382696,\n",
       "  0.26550867775363063,\n",
       "  0.2656535439317136,\n",
       "  0.2618676039367111,\n",
       "  0.26353569876560184,\n",
       "  0.2807023197844762,\n",
       "  0.26378988325125957,\n",
       "  0.19645439174092422,\n",
       "  0.23854945968961766,\n",
       "  0.25225540151872117],\n",
       " [0.30747025002285155,\n",
       "  0.24449547636268187,\n",
       "  0.22377596523052756,\n",
       "  0.25372028768495924,\n",
       "  0.26672805651831083,\n",
       "  0.23521748557524436,\n",
       "  0.2690572249949686,\n",
       "  0.16182274141779418,\n",
       "  0.18937452979005087,\n",
       "  0.33888542895992113,\n",
       "  0.2671940961273999,\n",
       "  0.24569344160088372,\n",
       "  0.2634506744649846,\n",
       "  0.23630114020818552,\n",
       "  0.26109307464456544,\n",
       "  0.32258223249261275,\n",
       "  0.24861462844180815,\n",
       "  0.32817351884304835,\n",
       "  0.34072770290643467],\n",
       " [0.19397006287910257,\n",
       "  0.17431871568694388,\n",
       "  0.18766015718687937,\n",
       "  0.232730330667497,\n",
       "  0.19891786708515713,\n",
       "  0.18294323827218342,\n",
       "  0.18530075405621593,\n",
       "  0.2148905368424912,\n",
       "  0.19404434339142837,\n",
       "  0.14772540904932002,\n",
       "  0.22997092375447845,\n",
       "  0.14872978890754712,\n",
       "  0.19765642860538257,\n",
       "  0.21497866143052186,\n",
       "  0.2103587477650624,\n",
       "  0.12532725010833967,\n",
       "  0.17832698856305054,\n",
       "  0.19854026533614255,\n",
       "  0.14191541049026885],\n",
       " [0.1840153216954208,\n",
       "  0.19376205870720406,\n",
       "  0.22853576048960997,\n",
       "  0.18135818454525007,\n",
       "  0.19571668737711065,\n",
       "  0.24595486870733216,\n",
       "  0.21590373958347855,\n",
       "  0.24006099661776203,\n",
       "  0.23699586603483858,\n",
       "  0.13814138243877708,\n",
       "  0.21292410721710106,\n",
       "  0.24146831026249724,\n",
       "  0.1443263592063561,\n",
       "  0.1761077243383066,\n",
       "  0.1335198179685753,\n",
       "  0.24039232200686858,\n",
       "  0.22169348241399597,\n",
       "  0.2347681881972177,\n",
       "  0.24085419219247137],\n",
       " [0.27074261258306725,\n",
       "  0.2760780265115246,\n",
       "  0.25017230853502953,\n",
       "  0.2564312090272281,\n",
       "  0.2779558091761245,\n",
       "  0.28030737763066726,\n",
       "  0.246279137808195,\n",
       "  0.2765682373754896,\n",
       "  0.21356148764795202,\n",
       "  0.07965589123910546,\n",
       "  0.20659701859816262,\n",
       "  0.20615886941626577,\n",
       "  0.2182879327972891,\n",
       "  0.29266796029442277,\n",
       "  0.2686726947803127,\n",
       "  0.09221755847085909,\n",
       "  0.2266790010130163,\n",
       "  0.2578234804021813,\n",
       "  0.2466460245263409],\n",
       " [0.41645076049426133,\n",
       "  0.4183558280710009,\n",
       "  0.4195895597478942,\n",
       "  0.4460978946565672,\n",
       "  0.4191402793347435,\n",
       "  0.44903282793188426,\n",
       "  0.4001701425163948,\n",
       "  0.28170586866795233,\n",
       "  0.41290828449706696,\n",
       "  0.318296999603698,\n",
       "  0.44799676007557765,\n",
       "  0.4459470061881081,\n",
       "  0.42082011106467976,\n",
       "  0.44758418671129657,\n",
       "  0.28698739223503383,\n",
       "  0.27046285481223165,\n",
       "  0.42166033647246715,\n",
       "  0.24747762845748503,\n",
       "  0.28236915752024294],\n",
       " [0.21100805852335552,\n",
       "  0.21506168533300973,\n",
       "  0.2971574480132389,\n",
       "  0.22901990567043257,\n",
       "  0.3100159472927577,\n",
       "  0.31023196569310435,\n",
       "  0.18194784178125625,\n",
       "  0.29897426402353083,\n",
       "  0.2092431667549332,\n",
       "  0.26005195013909504,\n",
       "  0.20701837992598893,\n",
       "  0.1489450348841344,\n",
       "  0.25308078907991177,\n",
       "  0.28773400585819287,\n",
       "  0.22756057152858422,\n",
       "  0.21855975350434367,\n",
       "  0.30051582791828735,\n",
       "  0.2034919458864515,\n",
       "  0.19757744886249765],\n",
       " [0.17845634588744094,\n",
       "  0.19513698339607283,\n",
       "  0.19452789634707643,\n",
       "  0.20436252716074543,\n",
       "  0.2111512021070461,\n",
       "  0.21169303103163226,\n",
       "  0.1787540510925889,\n",
       "  0.21425892312082387,\n",
       "  0.17788223633982617,\n",
       "  0.17995114903696122,\n",
       "  0.19140585652100361,\n",
       "  0.21122241806713618,\n",
       "  0.20170425062793773,\n",
       "  0.10328473045256914,\n",
       "  0.20111456949404397,\n",
       "  0.21440530294576743,\n",
       "  0.2070831446343006,\n",
       "  0.2102660102720632,\n",
       "  0.09141877848886291],\n",
       " [0.204901280083715,\n",
       "  0.1929993787022773,\n",
       "  0.250756047747786,\n",
       "  0.23442408811304224,\n",
       "  0.26653083196290794,\n",
       "  0.2294550254755727,\n",
       "  0.26827259014722477,\n",
       "  0.22687774333949373,\n",
       "  0.2557405436951779,\n",
       "  0.23007070548163513,\n",
       "  0.218899294032924,\n",
       "  0.18154747586523898,\n",
       "  0.22368593564634637,\n",
       "  0.08403207078639238,\n",
       "  0.26417825651317434,\n",
       "  0.23740198549385708,\n",
       "  0.2551537862699728,\n",
       "  0.26181572908495165,\n",
       "  0.21710498360815356],\n",
       " [0.2211368779829486,\n",
       "  0.20496557696555506,\n",
       "  0.20562291671192134,\n",
       "  0.22132601793083212,\n",
       "  0.2235072655366951,\n",
       "  0.2924228676555279,\n",
       "  0.26047757188569287,\n",
       "  0.34581920854634357,\n",
       "  0.2538243498337173,\n",
       "  0.2523103271085879,\n",
       "  0.11743826919199478,\n",
       "  0.21625239183164272,\n",
       "  0.25682220352732243,\n",
       "  0.2496631206991282,\n",
       "  0.29384821749436635,\n",
       "  0.29111853602386095,\n",
       "  0.30448819046351616,\n",
       "  0.24359067406468618,\n",
       "  0.3039285382448119],\n",
       " [0.18153906965028593,\n",
       "  0.20006184797837104,\n",
       "  0.21134444957776452,\n",
       "  0.24424424146136658,\n",
       "  0.13044217299608601,\n",
       "  0.24406760055792365,\n",
       "  0.16916295020329875,\n",
       "  0.2223583648285118,\n",
       "  0.24160286179540366,\n",
       "  0.18073407049853524,\n",
       "  0.22559888331555766,\n",
       "  0.2270435381644619,\n",
       "  0.18640052064279403,\n",
       "  0.22291691127839114,\n",
       "  0.1653400546171014,\n",
       "  0.22202958449989546,\n",
       "  0.2072118771556895,\n",
       "  0.23953300760444873,\n",
       "  0.2291548918014389],\n",
       " [0.2286446165323975,\n",
       "  0.23819176424521268,\n",
       "  0.219862398484874,\n",
       "  0.19643529415999722,\n",
       "  0.2511936627305671,\n",
       "  0.21738708591173878,\n",
       "  0.25182011147008015,\n",
       "  0.09841037120752225,\n",
       "  0.2045056431478611,\n",
       "  0.24344613561255105,\n",
       "  0.23555750799604297,\n",
       "  0.2480815183778475,\n",
       "  0.2094913949605023,\n",
       "  0.1956513742916818,\n",
       "  0.22975046874541014,\n",
       "  0.1612277929901117,\n",
       "  0.2799514175041377,\n",
       "  0.20843778007825267,\n",
       "  0.2232183850301607],\n",
       " [0.19406747908565927,\n",
       "  0.2138802600730245,\n",
       "  0.18998587925429666,\n",
       "  0.24488968170620468,\n",
       "  0.22533684981169366,\n",
       "  0.18335955291758047,\n",
       "  0.2215022362154334,\n",
       "  0.14704180667254005,\n",
       "  0.22297345763975637,\n",
       "  0.2367703189752048,\n",
       "  0.22347665266710598,\n",
       "  0.18268877318635077,\n",
       "  0.2162774430117027,\n",
       "  0.1995093657089486,\n",
       "  0.22059071868922542,\n",
       "  0.16821773305356819,\n",
       "  0.19012697388526356,\n",
       "  0.1885889702970798,\n",
       "  0.1978753044379507],\n",
       " [0.146037020728073,\n",
       "  0.18976743121942183,\n",
       "  0.18955937595524464,\n",
       "  0.15891455284724987,\n",
       "  0.2251910549948198,\n",
       "  0.2267118409289356,\n",
       "  0.2037846405464812,\n",
       "  0.1454244849451839,\n",
       "  0.15035991142988547,\n",
       "  0.1693042765591904,\n",
       "  0.19029022979631094,\n",
       "  0.1678065451630144,\n",
       "  0.21327082496817462,\n",
       "  0.2050944229020481,\n",
       "  0.1880218041104246,\n",
       "  0.22469729138139674,\n",
       "  0.20867216046697085,\n",
       "  0.20696587278755096,\n",
       "  0.21288798280591978],\n",
       " [0.20440149962505927,\n",
       "  0.20172386497375738,\n",
       "  0.03417393369245874,\n",
       "  0.051533590254904955,\n",
       "  0.20849487243671597,\n",
       "  0.20761713045848032,\n",
       "  0.03722841318453891,\n",
       "  0.19936339330768962,\n",
       "  0.20701400812556686,\n",
       "  0.20121074765029043,\n",
       "  0.20868170387695967,\n",
       "  0.037101238608040904,\n",
       "  0.20538076335043737,\n",
       "  0.20540589886546762,\n",
       "  0.034590744678822824,\n",
       "  0.20062222610626365,\n",
       "  0.20469468126882598,\n",
       "  0.21043572849066589,\n",
       "  0.029917440098657294],\n",
       " [0.1735995706633119,\n",
       "  0.16213812416331946,\n",
       "  0.25153737259106246,\n",
       "  0.1645124427556015,\n",
       "  0.24536187649033434,\n",
       "  0.23166115277559354,\n",
       "  0.1830271683840716,\n",
       "  0.23517314062189928,\n",
       "  0.182521374396721,\n",
       "  0.24059281869419222,\n",
       "  0.15616072451584795,\n",
       "  0.20112996190379437,\n",
       "  0.2326329206675923,\n",
       "  0.22130184091310834,\n",
       "  0.27448176397442553,\n",
       "  0.18637820506717914,\n",
       "  0.22524406123648047,\n",
       "  0.17408345628146094,\n",
       "  0.2605473077207404],\n",
       " [0.1763081042452423,\n",
       "  0.2215641254024738,\n",
       "  0.28052385077862735,\n",
       "  0.278727971902135,\n",
       "  0.2936465892488267,\n",
       "  0.26289927452895895,\n",
       "  0.25081631516418385,\n",
       "  0.2772823935854511,\n",
       "  0.267933216388668,\n",
       "  0.2152762153281513,\n",
       "  0.2127773470325822,\n",
       "  0.06486900434586129,\n",
       "  0.2312182051663287,\n",
       "  0.2520713957351951,\n",
       "  0.2426963578742324,\n",
       "  0.27647076372306517,\n",
       "  0.2873003912954322,\n",
       "  0.27490838581733573,\n",
       "  0.2593057464495933],\n",
       " [0.17864680840084274,\n",
       "  0.29744107361507083,\n",
       "  0.29227951512543393,\n",
       "  0.25653240317671394,\n",
       "  0.3000875092231003,\n",
       "  0.31011756897727194,\n",
       "  0.27071090784022495,\n",
       "  0.23051845801690202,\n",
       "  0.3035661209583564,\n",
       "  0.11790867659599359,\n",
       "  0.2785582701801656,\n",
       "  0.054330513539203644,\n",
       "  0.30252723159791095,\n",
       "  0.3043324469641822,\n",
       "  0.3108881446952171,\n",
       "  0.2863972294166948,\n",
       "  0.29594829005119605,\n",
       "  0.30655360723685116,\n",
       "  0.3082246479712338],\n",
       " [0.13407921432148223,\n",
       "  0.1940887528776615,\n",
       "  0.21113546129188537,\n",
       "  0.20654625995220707,\n",
       "  0.17336422172591698,\n",
       "  0.18897473015131092,\n",
       "  0.19438950903813793,\n",
       "  0.1926799810014321,\n",
       "  0.16809343081259734,\n",
       "  0.15320429170559288,\n",
       "  0.20208186701100395,\n",
       "  0.19427799279870867,\n",
       "  0.1991018429312252,\n",
       "  0.20835350491263113,\n",
       "  0.1808559896170974,\n",
       "  0.19446620883858276,\n",
       "  0.21450215255376787,\n",
       "  0.20448177012953903,\n",
       "  0.21704376178669305],\n",
       " [0.4891349406865017,\n",
       "  0.37498828627862096,\n",
       "  0.5408105645395073,\n",
       "  0.5472516641796321,\n",
       "  0.3811193177625823,\n",
       "  0.4060036625295189,\n",
       "  0.39480982046327523,\n",
       "  0.39829256071683017,\n",
       "  0.39526350689765527,\n",
       "  0.4159977101214709,\n",
       "  0.5224584408332239,\n",
       "  0.43843600546297234,\n",
       "  0.5215517009186418,\n",
       "  0.5327070554528212,\n",
       "  0.328536877824372,\n",
       "  0.39117516730955154,\n",
       "  0.32779100269316325,\n",
       "  0.3599053432038325,\n",
       "  0.3990919268233591],\n",
       " [0.16410361707428067,\n",
       "  0.1611341951247812,\n",
       "  0.197899352323193,\n",
       "  0.24543053316077937,\n",
       "  0.15766991607080763,\n",
       "  0.21751521719362762,\n",
       "  0.22889142922499436,\n",
       "  0.17410143570251604,\n",
       "  0.24117308608916663,\n",
       "  0.24242973763769027,\n",
       "  0.21237965174077542,\n",
       "  0.23699408072661904,\n",
       "  0.19466857313839345,\n",
       "  0.16048663883816267,\n",
       "  0.17175656856646995,\n",
       "  0.24143427930519987,\n",
       "  0.20675068571609764,\n",
       "  0.2422590248786462,\n",
       "  0.2257161350003386],\n",
       " [0.31112836890858303,\n",
       "  0.32013489260398703,\n",
       "  0.30927105289468787,\n",
       "  0.3225169859049493,\n",
       "  0.3880119136635479,\n",
       "  0.15811431844040427,\n",
       "  0.333509474109817,\n",
       "  0.3850621827753843,\n",
       "  0.30673313640645544,\n",
       "  0.3067531480613821,\n",
       "  0.2951522215255897,\n",
       "  0.3122868653697805,\n",
       "  0.3384997024547194,\n",
       "  0.2476484243104305,\n",
       "  0.3834234418343905,\n",
       "  0.3167760200133413,\n",
       "  0.3599256792612482,\n",
       "  0.29136825141111705,\n",
       "  0.3874949594147892],\n",
       " [0.24998752788412235,\n",
       "  0.2630494669329822,\n",
       "  0.24234790948838547,\n",
       "  0.21707514251323562,\n",
       "  0.1925053928388332,\n",
       "  0.2961634421336575,\n",
       "  0.2072623562130258,\n",
       "  0.1885810341464128,\n",
       "  0.2586601106881215,\n",
       "  0.2675773652458743,\n",
       "  0.1675722883052673,\n",
       "  0.19749711127483213,\n",
       "  0.12398858945283002,\n",
       "  0.21938078454992185,\n",
       "  0.1846677599835776,\n",
       "  0.17384816934045524,\n",
       "  0.22027531538690376,\n",
       "  0.190421469747473,\n",
       "  0.28519811726719047],\n",
       " [0.3891139468606091,\n",
       "  0.36417330433778256,\n",
       "  0.3662400733675859,\n",
       "  0.3843830098639745,\n",
       "  0.4138853456433834,\n",
       "  0.1798559340139741,\n",
       "  0.36802405584931536,\n",
       "  0.4150473891895705,\n",
       "  0.3297840174784852,\n",
       "  0.2570515843249425,\n",
       "  0.37914124247684294,\n",
       "  0.3954743503437504,\n",
       "  0.41190749240006386,\n",
       "  0.06548591436548348,\n",
       "  0.3635774261545191,\n",
       "  0.37685934947194477,\n",
       "  0.33125848719378514,\n",
       "  0.4148203448676532,\n",
       "  0.32259564190176554],\n",
       " [0.4679626590068873,\n",
       "  0.5590384369749946,\n",
       "  0.15656565824306296,\n",
       "  0.3243239233931637,\n",
       "  0.25818568601518677,\n",
       "  0.36903053803788927,\n",
       "  0.5387217576906166,\n",
       "  0.4496172664864673,\n",
       "  0.5735418550295915,\n",
       "  0.5757985845220126,\n",
       "  0.22623588387945978,\n",
       "  0.5444728091901699,\n",
       "  0.4918187186214388,\n",
       "  0.46192183671062015,\n",
       "  0.4720968467975003,\n",
       "  0.4816358428269227,\n",
       "  0.5646075120439055,\n",
       "  0.5719204271838292,\n",
       "  0.490801416971796],\n",
       " [0.422335436085269,\n",
       "  0.41764651032251665,\n",
       "  0.3844706373311772,\n",
       "  0.41874810909997584,\n",
       "  0.10058880081872247,\n",
       "  0.3094296193974249,\n",
       "  0.18527246676153913,\n",
       "  0.321325265475371,\n",
       "  0.39521762589193704,\n",
       "  0.4061171188692372,\n",
       "  0.4443100804492229,\n",
       "  0.43024182485620205,\n",
       "  0.3532233503046087,\n",
       "  0.4149227597425526,\n",
       "  0.36533010157830437,\n",
       "  0.3935129692303631,\n",
       "  0.24803832291691563,\n",
       "  0.3232318793280409,\n",
       "  0.3658424036517139],\n",
       " [0.1821904814167235,\n",
       "  0.12963575115607623,\n",
       "  0.19662335421928873,\n",
       "  0.18964082399770238,\n",
       "  0.1697331773305989,\n",
       "  0.1943583643525752,\n",
       "  0.21363251479948653,\n",
       "  0.23483040926686524,\n",
       "  0.16236869622434363,\n",
       "  0.1986070579565646,\n",
       "  0.24985260728520345,\n",
       "  0.20103134324209535,\n",
       "  0.18836817859068197,\n",
       "  0.1850810464932231,\n",
       "  0.20095263210812403,\n",
       "  0.1927751173695462,\n",
       "  0.18615037455564068,\n",
       "  0.18809590613141977,\n",
       "  0.1757196877804428],\n",
       " [0.25085781750500685,\n",
       "  0.2177201806589688,\n",
       "  0.2635013032077168,\n",
       "  0.19566974375501908,\n",
       "  0.26902917833298057,\n",
       "  0.2163859097283314,\n",
       "  0.25527032381899634,\n",
       "  0.1318719773558916,\n",
       "  0.19163958891095537,\n",
       "  0.2335320662404577,\n",
       "  0.2585390251000817,\n",
       "  0.2366142458779241,\n",
       "  0.2729184115827424,\n",
       "  0.25830375141908274,\n",
       "  0.2906212806390159,\n",
       "  0.15650495475047485,\n",
       "  0.20271556700250207,\n",
       "  0.1428627101149537,\n",
       "  0.18917283555427464],\n",
       " [0.22727460999518634,\n",
       "  0.212112279375024,\n",
       "  0.2718886397183104,\n",
       "  0.1974321778415277,\n",
       "  0.2829506381130979,\n",
       "  0.25852205972472897,\n",
       "  0.25225670826096463,\n",
       "  0.21119910436526862,\n",
       "  0.21649456313999335,\n",
       "  0.21303141116951263,\n",
       "  0.18334029513864317,\n",
       "  0.19235272597922515,\n",
       "  0.25880346570159823,\n",
       "  0.2548234819366811,\n",
       "  0.27533618752252115,\n",
       "  0.27122335278267445,\n",
       "  0.2733401035363261,\n",
       "  0.23582073618005783,\n",
       "  0.2851579425857454],\n",
       " [0.24477530651781007,\n",
       "  0.2659662514190182,\n",
       "  0.20410425663574355,\n",
       "  0.21507428909550477,\n",
       "  0.1427524752744405,\n",
       "  0.2177537798469789,\n",
       "  0.24233105844709632,\n",
       "  0.21324679955336948,\n",
       "  0.24649581162269454,\n",
       "  0.153528822696939,\n",
       "  0.15428049637317243,\n",
       "  0.1880864280984871,\n",
       "  0.20483027119967617,\n",
       "  0.18296412331237505,\n",
       "  0.2710427047082893,\n",
       "  0.1980165383910814,\n",
       "  0.2391135719135984,\n",
       "  0.10008514019766267,\n",
       "  0.21949526475640133],\n",
       " [0.2591699989108875,\n",
       "  0.31380049005727545,\n",
       "  0.2877787066899414,\n",
       "  0.30188674745468924,\n",
       "  0.3391158510159725,\n",
       "  0.13569845551254542,\n",
       "  0.26749825693145246,\n",
       "  0.3171453314241022,\n",
       "  0.18056761645993308,\n",
       "  0.3515948513132105,\n",
       "  0.2557376352251651,\n",
       "  0.31750239290508986,\n",
       "  0.2650724533809373,\n",
       "  0.32349406942073017,\n",
       "  0.34045966532822836,\n",
       "  0.2998923027412953,\n",
       "  0.3490326617068707,\n",
       "  0.3072331573988176,\n",
       "  0.3426768730938616],\n",
       " [0.23145750370245213,\n",
       "  0.19952790143365995,\n",
       "  0.24694985079371484,\n",
       "  0.23476471238903812,\n",
       "  0.24966168492047752,\n",
       "  0.22533796477531615,\n",
       "  0.22576444832821832,\n",
       "  0.18152059019565978,\n",
       "  0.2589479359596418,\n",
       "  0.15790642097974034,\n",
       "  0.243500496982941,\n",
       "  0.17528887834971732,\n",
       "  0.2765293385564304,\n",
       "  0.2562435944476259,\n",
       "  0.28980794513666336,\n",
       "  0.1981995287985999,\n",
       "  0.23550758749669287,\n",
       "  0.2335199788363404,\n",
       "  0.1701819809456942],\n",
       " [0.31324179517586254,\n",
       "  0.33961346194617487,\n",
       "  0.2513207326798076,\n",
       "  0.3253690503364441,\n",
       "  0.19957238239259395,\n",
       "  0.25557192412675606,\n",
       "  0.25597457547382196,\n",
       "  0.31225307662910573,\n",
       "  0.3052561981325624,\n",
       "  0.25428973113665815,\n",
       "  0.2021397067054075,\n",
       "  0.3218117857298069,\n",
       "  0.3106761661131985,\n",
       "  0.13988437566496065,\n",
       "  0.31446882249724395,\n",
       "  0.1839262138021736,\n",
       "  0.3522041155671461,\n",
       "  0.3419572947558577,\n",
       "  0.32155256710010177],\n",
       " [0.1442126172986282,\n",
       "  0.19085340834972528,\n",
       "  0.22083548215950866,\n",
       "  0.19856972221095162,\n",
       "  0.22380188794478906,\n",
       "  0.27684653699595885,\n",
       "  0.21031005858625104,\n",
       "  0.18400902110400955,\n",
       "  0.27755503334996334,\n",
       "  0.29701724792921186,\n",
       "  0.20806045063397355,\n",
       "  0.2087550160781188,\n",
       "  0.2112416730800961,\n",
       "  0.2643191108147663,\n",
       "  0.2222320947229314,\n",
       "  0.2251508578230658,\n",
       "  0.1104004891133979,\n",
       "  0.2439706836893774,\n",
       "  0.1895110065737434],\n",
       " [0.146095374876803,\n",
       "  0.14691517647041025,\n",
       "  0.24800246414237992,\n",
       "  0.11175827756653887,\n",
       "  0.25120051154908934,\n",
       "  0.23875768578112327,\n",
       "  0.251771473888883,\n",
       "  0.19382523625545656,\n",
       "  0.2508285659039679,\n",
       "  0.21795917009605748,\n",
       "  0.16024704103928505,\n",
       "  0.207434548355236,\n",
       "  0.22823215241476197,\n",
       "  0.1551527148420099,\n",
       "  0.15773768207987354,\n",
       "  0.22762557096384003,\n",
       "  0.2481567220489314,\n",
       "  0.2353077901654271,\n",
       "  0.16805166791001794],\n",
       " [0.1783128890714002,\n",
       "  0.19996567384629468,\n",
       "  0.23493270030205812,\n",
       "  0.21942368090450395,\n",
       "  0.22145489114121925,\n",
       "  0.1515911517765282,\n",
       "  0.24300211072362204,\n",
       "  0.25962534661726294,\n",
       "  0.18347017356447926,\n",
       "  0.16782651568578058,\n",
       "  0.2326871194585296,\n",
       "  0.21337602995440344,\n",
       "  0.255246563407626,\n",
       "  0.17479020218651906,\n",
       "  0.22995953657236598,\n",
       "  0.1748797512817287,\n",
       "  0.21321416720951372,\n",
       "  0.20396189430032477,\n",
       "  0.20712650661575457],\n",
       " [0.14335411461621037,\n",
       "  0.11431873712630493,\n",
       "  0.2492939942788905,\n",
       "  0.2414954279694921,\n",
       "  0.23003526765086704,\n",
       "  0.2184827744765686,\n",
       "  0.20871688881895906,\n",
       "  0.16577762984741828,\n",
       "  0.18739739491592808,\n",
       "  0.2317105247671405,\n",
       "  0.21302123171221427,\n",
       "  0.21276558874865842,\n",
       "  0.2447922506704293,\n",
       "  0.24243199953382846,\n",
       "  0.2238848728491775,\n",
       "  0.1644906906031036,\n",
       "  0.22785100349061294,\n",
       "  0.2516654032936151,\n",
       "  0.24030688158400612],\n",
       " [0.17837436486053507,\n",
       "  0.2561503989664269,\n",
       "  0.2835927729783377,\n",
       "  0.18843022161078532,\n",
       "  0.26260575609227793,\n",
       "  0.2010968384606208,\n",
       "  0.2808907961297225,\n",
       "  0.2615033296781029,\n",
       "  0.15638597542643806,\n",
       "  0.35721346240465085,\n",
       "  0.14788982697404612,\n",
       "  0.3280793127952413,\n",
       "  0.3085893160660036,\n",
       "  0.18910216875016508,\n",
       "  0.30678294661295136,\n",
       "  0.18221710066579752,\n",
       "  0.16623047009511416,\n",
       "  0.2597221462652528,\n",
       "  0.27086693358504077],\n",
       " [0.20840360850367812,\n",
       "  0.20290709838981855,\n",
       "  0.023490144051441765,\n",
       "  0.21752399231971178,\n",
       "  0.2213316816644466,\n",
       "  0.22101043668574596,\n",
       "  0.22109980684780606,\n",
       "  0.22124915809883688,\n",
       "  0.028537597373736998,\n",
       "  0.02489434558726526,\n",
       "  0.21848706670661303,\n",
       "  0.018539455289806567,\n",
       "  0.22155558000953843,\n",
       "  0.021511880304621936,\n",
       "  0.02681595347746311,\n",
       "  0.2207106070448711,\n",
       "  0.2214597086467321,\n",
       "  0.2213947345187954,\n",
       "  0.22012636788471157],\n",
       " [0.19938018147566036,\n",
       "  0.1909442332630957,\n",
       "  0.2710260246858052,\n",
       "  0.18619033644325592,\n",
       "  0.26740282846129976,\n",
       "  0.12986588540269106,\n",
       "  0.20166456331363475,\n",
       "  0.13499868812783603,\n",
       "  0.2643905812861602,\n",
       "  0.11448505708098623,\n",
       "  0.2410732075370187,\n",
       "  0.16329723284436926,\n",
       "  0.26906762245823185,\n",
       "  0.22685630109113072,\n",
       "  0.24219075020916175,\n",
       "  0.23004460385457667,\n",
       "  0.2265126529205838,\n",
       "  0.20292300540932717,\n",
       "  0.17278796813567204],\n",
       " [0.304194928644019,\n",
       "  0.27071783300618424,\n",
       "  0.28895251441955594,\n",
       "  0.32811949317394634,\n",
       "  0.2851395009804568,\n",
       "  0.2154849139805598,\n",
       "  0.30935253802734536,\n",
       "  0.14966255830046601,\n",
       "  0.31014833939630637,\n",
       "  0.2983589419939901,\n",
       "  0.31007475723451544,\n",
       "  0.3202997810210923,\n",
       "  0.20443452075778912,\n",
       "  0.2925768121909389,\n",
       "  0.20188473440233753,\n",
       "  0.1585620655065826,\n",
       "  0.3195126734583299,\n",
       "  0.22793392350998756,\n",
       "  0.2619497523407585],\n",
       " [0.29004863628164906,\n",
       "  0.31337816069921504,\n",
       "  0.2882984389458045,\n",
       "  0.27351529228833577,\n",
       "  0.3276852859040554,\n",
       "  0.266055279333359,\n",
       "  0.3840357279809691,\n",
       "  0.41019928788750376,\n",
       "  0.4313004425150131,\n",
       "  0.39483176398344416,\n",
       "  0.12249497996296017,\n",
       "  0.10921930245750625,\n",
       "  0.3048130524481881,\n",
       "  0.2770244316974221,\n",
       "  0.3456973588728744,\n",
       "  0.3757361896349838,\n",
       "  0.3323133955704578,\n",
       "  0.31355082981723914,\n",
       "  0.4057690519423511],\n",
       " [0.3134191553658876,\n",
       "  0.3962768538108622,\n",
       "  0.32477521789723623,\n",
       "  0.40409014162745777,\n",
       "  0.3940937927023689,\n",
       "  0.3824988170866755,\n",
       "  0.2795971238103496,\n",
       "  0.3182427733311601,\n",
       "  0.406030949556325,\n",
       "  0.24554564398374548,\n",
       "  0.1513756507153572,\n",
       "  0.371505592300664,\n",
       "  0.24945363922248837,\n",
       "  0.39228775680940486,\n",
       "  0.3981933419425564,\n",
       "  0.17151901876462372,\n",
       "  0.40687918926765115,\n",
       "  0.37190765069909826,\n",
       "  0.4035064433522554],\n",
       " [0.1536919866115549,\n",
       "  0.1493037798621568,\n",
       "  0.24430102177937096,\n",
       "  0.1762148148549479,\n",
       "  0.23633461396983932,\n",
       "  0.1931908423274067,\n",
       "  0.1678943298290998,\n",
       "  0.1874094273671018,\n",
       "  0.23525193065312153,\n",
       "  0.21840231595067353,\n",
       "  0.16812138379649352,\n",
       "  0.1868310450782411,\n",
       "  0.22662781675544824,\n",
       "  0.20437501884715314,\n",
       "  0.1446040281570778,\n",
       "  0.1579777593139808,\n",
       "  0.20236307450849098,\n",
       "  0.2734107204652473,\n",
       "  0.23325585408309532],\n",
       " [0.29634912631077087,\n",
       "  0.1540464753803668,\n",
       "  0.3485889419744916,\n",
       "  0.29399060578265757,\n",
       "  0.335455931505035,\n",
       "  0.24797698699350026,\n",
       "  0.22431272911236302,\n",
       "  0.23397805869251795,\n",
       "  0.26400673393116925,\n",
       "  0.31300566173513467,\n",
       "  0.32675352987667666,\n",
       "  0.2826055566829112,\n",
       "  0.3294183943202214,\n",
       "  0.3139308399213651,\n",
       "  0.35830286263816535,\n",
       "  0.2134365438455164,\n",
       "  0.3517975743072912,\n",
       "  0.3194542464331508,\n",
       "  0.20525641584344798],\n",
       " [0.21549691167292323,\n",
       "  0.2888552651508667,\n",
       "  0.2307410165622905,\n",
       "  0.24312046616531702,\n",
       "  0.24389915139123466,\n",
       "  0.28012110709549337,\n",
       "  0.33738404450666565,\n",
       "  0.1668520964678352,\n",
       "  0.317810113611206,\n",
       "  0.17904343525175456,\n",
       "  0.23765497334899854,\n",
       "  0.18645234773639888,\n",
       "  0.18353483608020804,\n",
       "  0.22854321264335026,\n",
       "  0.2131168276410407,\n",
       "  0.2291552297973311,\n",
       "  0.17725427798209503,\n",
       "  0.2431212432093526,\n",
       "  0.2875798763856161],\n",
       " [0.1254809396696112,\n",
       "  0.23171416904731482,\n",
       "  0.2530931140516877,\n",
       "  0.13011409949040534,\n",
       "  0.19052293779488175,\n",
       "  0.26310001911518427,\n",
       "  0.2860889731186099,\n",
       "  0.23771007789635376,\n",
       "  0.24491021220186968,\n",
       "  0.2935814515796892,\n",
       "  0.1766727743288606,\n",
       "  0.24364514798130912,\n",
       "  0.22909392074624113,\n",
       "  0.2320954415001282,\n",
       "  0.20092265284313085,\n",
       "  0.2426027388233505,\n",
       "  0.20900821975569875,\n",
       "  0.2632245441031642,\n",
       "  0.23039737780095337],\n",
       " [0.24756648756494593,\n",
       "  0.24311971907914176,\n",
       "  0.18988591229654442,\n",
       "  0.2792536579858332,\n",
       "  0.21742928997397698,\n",
       "  0.13545235300865105,\n",
       "  0.24102757140953485,\n",
       "  0.25114990760694844,\n",
       "  0.3200898662893046,\n",
       "  0.2887733955776999,\n",
       "  0.26769494478429107,\n",
       "  0.2701508999734049,\n",
       "  0.2715617594360527,\n",
       "  0.1926547545547448,\n",
       "  0.24347898966217818,\n",
       "  0.25766347689738045,\n",
       "  0.20642713191844794,\n",
       "  0.18435858631382473,\n",
       "  0.24548468402305745],\n",
       " [0.22083836156119832,\n",
       "  0.21787418333099418,\n",
       "  0.059061755142131585,\n",
       "  0.22139025985621993,\n",
       "  0.221190493293177,\n",
       "  0.2213021333889234,\n",
       "  0.047365489413216756,\n",
       "  0.1969319122134037,\n",
       "  0.05651214064020712,\n",
       "  0.2147349583163657,\n",
       "  0.0645025478667971,\n",
       "  0.21878535828283774,\n",
       "  0.21701229830686306,\n",
       "  0.18938365885359315,\n",
       "  0.21668788784066664,\n",
       "  0.21148279469205492,\n",
       "  0.21997837871196818,\n",
       "  0.2190775127180566,\n",
       "  0.051577438340355095],\n",
       " [0.2298351885969995,\n",
       "  0.22944467705532617,\n",
       "  0.25355229038789606,\n",
       "  0.25549248317781265,\n",
       "  0.2606702760940718,\n",
       "  0.2960340820027095,\n",
       "  0.20182212256026713,\n",
       "  0.2392924745949302,\n",
       "  0.22018790977894512,\n",
       "  0.08563094758168768,\n",
       "  0.24385804139054834,\n",
       "  0.13224929042165265,\n",
       "  0.28703694118827927,\n",
       "  0.28011279855056964,\n",
       "  0.2503935282207316,\n",
       "  0.2331405633434423,\n",
       "  0.24535283510554198,\n",
       "  0.2449900824122041,\n",
       "  0.26444018226563215],\n",
       " [0.27356360115734785,\n",
       "  0.26658686628188594,\n",
       "  0.31965027325440015,\n",
       "  0.23087663184522075,\n",
       "  0.30728869976508755,\n",
       "  0.19999443516484589,\n",
       "  0.29642306051993145,\n",
       "  0.27735524055955885,\n",
       "  0.13793041752571844,\n",
       "  0.24722357551371357,\n",
       "  0.3066364485148619,\n",
       "  0.29788081486953627,\n",
       "  0.3171119584494913,\n",
       "  0.2787829072233092,\n",
       "  0.31086972489393017,\n",
       "  0.17858394074534203,\n",
       "  0.29207328041477354,\n",
       "  0.3147860737336043,\n",
       "  0.2288576755535736],\n",
       " [0.28533471639814145,\n",
       "  0.29016868675709595,\n",
       "  0.26887808816079584,\n",
       "  0.23356942738356284,\n",
       "  0.2729725286826847,\n",
       "  0.23168584924715122,\n",
       "  0.2676921971941025,\n",
       "  0.25501745411299437,\n",
       "  0.3261819497796903,\n",
       "  0.28651812166635027,\n",
       "  0.1995337224665484,\n",
       "  0.25087165110478477,\n",
       "  0.23238089876212892,\n",
       "  0.24812749873595114,\n",
       "  0.2222215335642626,\n",
       "  0.23866649797517864,\n",
       "  0.21489523529353338,\n",
       "  0.18801673491277368,\n",
       "  0.21420158081869753],\n",
       " [0.1926963943542729,\n",
       "  0.2055047417089089,\n",
       "  0.24059780118755492,\n",
       "  0.22583580069749848,\n",
       "  0.18429350912799491,\n",
       "  0.2422117701320828,\n",
       "  0.1985421175056859,\n",
       "  0.20502197349078724,\n",
       "  0.17232712881492496,\n",
       "  0.12037305500019375,\n",
       "  0.18897964739385595,\n",
       "  0.31424319438121007,\n",
       "  0.10591214355479195,\n",
       "  0.1439050380502686,\n",
       "  0.2438514412531835,\n",
       "  0.2570937175566038,\n",
       "  0.25160385488668013,\n",
       "  0.2425834394362824,\n",
       "  0.21020170861844903],\n",
       " [0.368586785965923,\n",
       "  0.34097416719776996,\n",
       "  0.5655049905153225,\n",
       "  0.5271765161063038,\n",
       "  0.5443976071714978,\n",
       "  0.42338011663633474,\n",
       "  0.2482782624386034,\n",
       "  0.5799710391504247,\n",
       "  0.6089720100392019,\n",
       "  0.489032688647952,\n",
       "  0.32210734260692514,\n",
       "  0.17120729003518706,\n",
       "  0.38151491503174206,\n",
       "  0.5754163376289135,\n",
       "  0.6105234897837064,\n",
       "  0.5970342026126745,\n",
       "  0.6384940964975958,\n",
       "  0.47912054708932217,\n",
       "  0.26670124284018504],\n",
       " [0.12568167982219977,\n",
       "  0.17654393525978662,\n",
       "  0.17788298464541571,\n",
       "  0.12613497637065524,\n",
       "  0.1770445335343559,\n",
       "  0.15666103753108576,\n",
       "  0.1629087248482518,\n",
       "  0.14125351557163357,\n",
       "  0.1956403342127488,\n",
       "  0.17995247346703497,\n",
       "  0.11294116835637528,\n",
       "  0.17296977627962476,\n",
       "  0.19972806403600118,\n",
       "  0.1895324739629048,\n",
       "  0.19082050605478923,\n",
       "  0.1803581396033186,\n",
       "  0.18437211739009254,\n",
       "  0.19140586204772395,\n",
       "  0.17911061252070162],\n",
       " [0.15234442399868237,\n",
       "  0.09376169143193863,\n",
       "  0.13737997940444563,\n",
       "  0.20294322273277315,\n",
       "  0.18350233567985932,\n",
       "  0.20152903043239576,\n",
       "  0.1698833145610428,\n",
       "  0.13203843971417092,\n",
       "  0.09949378219413034,\n",
       "  0.18544577792093803,\n",
       "  0.0824580947527219,\n",
       "  0.19395718241709772,\n",
       "  0.20576280130563918,\n",
       "  0.20027659825009808,\n",
       "  0.20571529939588684,\n",
       "  0.1224305965451708,\n",
       "  0.1309128931234908,\n",
       "  0.19585740149064698,\n",
       "  0.19859172114681042],\n",
       " [0.1361674048289309,\n",
       "  0.1440395141184946,\n",
       "  0.19884071087244273,\n",
       "  0.2038246962498363,\n",
       "  0.20658157413069048,\n",
       "  0.11437960105487333,\n",
       "  0.19720334041262966,\n",
       "  0.20304458871632958,\n",
       "  0.10751712890590116,\n",
       "  0.11535512632067811,\n",
       "  0.20005559785998014,\n",
       "  0.10846327954114547,\n",
       "  0.11104077563593019,\n",
       "  0.19243481103176083,\n",
       "  0.19311369999405326,\n",
       "  0.1843274497762399,\n",
       "  0.11565127695284623,\n",
       "  0.17498645289865536,\n",
       "  0.20566303531197394]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.17      0.29        12\n",
      "           1       0.28      1.00      0.44         7\n",
      "           2       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.33        27\n",
      "   macro avg       0.43      0.39      0.24        27\n",
      "weighted avg       0.52      0.33      0.24        27\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [data.label for data in train_list]\n",
    "y_test = [data.label for data in test_list]\n",
    "\n",
    "x_train = [data.x[5] for data in train_list]\n",
    "x_test = [data.x[5] for data in test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 2.11339934e-05,  6.66159354e-05,  4.43768285e-09, -8.20041280e+00,\n",
       "         5.58208392e-02, -2.12391052e-02]),\n",
       " array([ 1.24992132e-04,  8.33498491e-04,  6.94719734e-07, -7.10161292e+00,\n",
       "         7.67759604e-01,  9.15164056e+00]),\n",
       " array([-3.64692359e-06,  6.79034848e-05,  4.61088325e-09, -8.18739250e+00,\n",
       "         2.76200788e-01,  2.03062794e-01]),\n",
       " array([ 2.27730484e-05,  7.42379007e-05,  5.51126590e-09, -8.13365462e+00,\n",
       "         8.80167850e-01,  8.04961871e+00]),\n",
       " array([-3.26333894e-05,  6.33436299e-05,  4.01241545e-09, -8.24604488e+00,\n",
       "        -1.11824856e-01,  2.74494088e-02]),\n",
       " array([ 2.05007424e-05,  6.10986913e-05,  3.73305007e-09,            -inf,\n",
       "        -7.15941882e-02,  2.79303348e-02]),\n",
       " array([ 7.02959125e-05,  6.84787335e-05,  4.68933695e-09, -8.17454264e+00,\n",
       "         4.26923682e-02,  5.24417393e-02]),\n",
       " array([ 2.58139689e-05,  6.52081087e-05,  4.25209744e-09, -8.22121934e+00,\n",
       "        -1.26370575e-01,  9.44925941e-02]),\n",
       " array([-1.00885318e-05,  8.08517803e-05,  6.53701037e-09, -8.19921267e+00,\n",
       "         2.26677983e+00,  1.13525142e+02]),\n",
       " array([ 1.72345130e-06,  2.30774584e-04,  5.32569088e-08, -8.06832375e+00,\n",
       "        -1.00253413e+01,  1.24085277e+02]),\n",
       " array([ 4.33645449e-05,  1.13671050e-04,  1.29211076e-08, -7.99134542e+00,\n",
       "         3.81202166e+00,  6.27719795e+01]),\n",
       " array([-4.02394197e-06,  5.98192235e-05,  3.57833950e-09, -8.30656096e+00,\n",
       "         4.47615961e-02, -1.58085598e-01]),\n",
       " array([-4.71120371e-05,  6.78058920e-05,  4.59763899e-09, -8.21379845e+00,\n",
       "         1.16686348e+00,  2.03914536e+01]),\n",
       " array([ 6.40507547e-05,  7.02924976e-05,  4.94103522e-09, -8.14866510e+00,\n",
       "         1.79720918e-01, -1.68509185e-02]),\n",
       " array([-2.70554598e-05,  9.07305077e-05,  8.23202503e-09, -7.89775743e+00,\n",
       "         2.12744170e-01,  3.31045122e-03]),\n",
       " array([ 8.04317012e-05,  7.06501420e-05,  4.99144256e-09, -8.14025004e+00,\n",
       "         1.34717133e-01,  2.15980577e-01]),\n",
       " array([ 2.88211144e-05,  6.44281268e-05,  4.15098352e-09, -8.23221658e+00,\n",
       "        -6.16179057e-03,  6.56309246e-02]),\n",
       " array([ 1.21492699e-05,  9.45055072e-05,  8.93129088e-09, -7.89175472e+00,\n",
       "        -5.27523645e-01,  2.38369878e+00]),\n",
       " array([-8.90708186e-05,  1.52022489e-04,  2.31108370e-08, -7.51146195e+00,\n",
       "         5.56039595e-01,  2.70491569e+01]),\n",
       " array([-5.40038605e-05,  1.01771783e-04,  1.03574959e-08, -8.06828184e+00,\n",
       "        -3.67663999e+00,  2.64402853e+01]),\n",
       " array([ 3.20393088e-05,  6.21326685e-05,  3.86046849e-09, -8.26915240e+00,\n",
       "         9.44598528e-02,  5.90839897e-02]),\n",
       " array([-2.78099447e-05,  1.73214970e-04,  3.00034258e-08, -8.05258993e+00,\n",
       "        -1.51360909e+00,  1.39015450e+02]),\n",
       " array([-4.00739869e-05,  6.39171251e-05,  4.08539888e-09, -8.23997930e+00,\n",
       "        -2.75723115e-02, -8.97829199e-02]),\n",
       " array([-1.96092843e-06,  1.33399631e-04,  1.77954614e-08, -7.83792441e+00,\n",
       "        -2.88659542e+00,  1.54864581e+01]),\n",
       " array([ 2.47057491e-05,  6.28365899e-05,  3.94843703e-09, -8.25744921e+00,\n",
       "        -1.94769240e-02, -8.99666483e-02]),\n",
       " array([6.93609081e-05, 1.57701127e-04, 2.48696455e-08,           -inf,\n",
       "        9.79616814e-04, 2.13892315e+00]),\n",
       " array([ 1.14123029e-04,  8.62601500e-05,  7.44081348e-09, -7.94113988e+00,\n",
       "         3.37098302e-04, -2.68167303e-02]),\n",
       " array([ 5.02049982e-05,  1.63604039e-04,  2.67662817e-08,            -inf,\n",
       "        -2.19732485e-02,  1.87171379e+00]),\n",
       " array([ 5.83911714e-05,  1.13712865e-04,  1.29306156e-08, -7.99629274e+00,\n",
       "        -2.22748923e+00,  2.58218770e+01]),\n",
       " array([-1.97024928e-04,  1.01282544e-04,  1.02581538e-08, -7.80314009e+00,\n",
       "        -5.10012743e-01,  4.11623128e-01]),\n",
       " array([-2.04509798e-05,  6.94337124e-05,  4.82104042e-09, -8.15922002e+00,\n",
       "         3.78171046e-02, -1.52590424e-01]),\n",
       " array([-2.59587830e-05,  6.34518882e-05,  4.02614212e-09, -8.24934697e+00,\n",
       "        -1.25482366e-01, -1.13238740e-02]),\n",
       " array([ 2.11672099e-05,  6.64545618e-05,  4.41620878e-09, -8.21119015e+00,\n",
       "         1.40726703e-02,  6.06300646e-01]),\n",
       " array([-1.44608635e-05,  6.38883524e-05,  4.08172157e-09, -8.24232643e+00,\n",
       "        -5.66271046e-02,  1.53909594e-01]),\n",
       " array([-1.70392116e-06,  1.08541615e-04,  1.17812822e-08, -7.72862471e+00,\n",
       "        -4.30323792e-01,  2.81993549e-01]),\n",
       " array([-1.20184818e-05,  6.54322461e-05,  4.28137883e-09, -8.21801178e+00,\n",
       "        -2.00129253e-03,  7.42242518e-02]),\n",
       " array([ 4.31769708e-05,  2.33437229e-04,  5.44929401e-08,            -inf,\n",
       "        -4.46531930e+00,  4.57479900e+01]),\n",
       " array([-3.50204017e-05,  6.52917841e-05,  4.26301707e-09, -8.22019375e+00,\n",
       "        -3.55829806e-02, -1.32630221e-02]),\n",
       " array([ 7.71996078e-05,  5.41034752e-04,  2.92718603e-07, -7.87844825e+00,\n",
       "         4.97699908e+00,  2.45115527e+01]),\n",
       " array([-5.75777391e-05,  7.52287782e-05,  5.65936907e-09, -8.07725249e+00,\n",
       "        -1.77910346e-02, -1.01286017e-01]),\n",
       " array([-7.20977412e-05,  6.21089496e-05,  3.85752163e-09, -8.27002697e+00,\n",
       "         4.58813393e-02, -8.33762544e-02]),\n",
       " array([ 1.44444470e-05,  6.47688128e-05,  4.19499911e-09, -8.22970386e+00,\n",
       "        -6.35234468e-02, -1.63696773e-01]),\n",
       " array([-2.99954066e-04,  8.36235702e-04,  6.99290150e-07, -7.52453958e+00,\n",
       "        -2.80323163e+00,  6.44943851e+00]),\n",
       " array([ 1.81300109e-05,  7.63401798e-05,  5.82782305e-09, -8.07241927e+00,\n",
       "         3.29095946e-01,  2.77797182e-01]),\n",
       " array([-4.33670354e-05,  6.42861477e-05,  4.13270879e-09, -8.23683637e+00,\n",
       "        -9.33203970e-02,  1.09590552e-01]),\n",
       " array([ 3.36483792e-05,  7.38184619e-05,  5.44916531e-09, -8.17289654e+00,\n",
       "         2.24627434e-01,  1.63897976e+01]),\n",
       " array([ 1.28692836e-04,  2.18665860e-04,  4.78147583e-08, -7.25330958e+00,\n",
       "        -1.29694075e+00,  1.39569998e+00]),\n",
       " array([-1.21230168e-05,  6.85626164e-05,  4.70083237e-09, -8.16910931e+00,\n",
       "         6.13824588e-03,  3.34125882e-01]),\n",
       " array([ 1.53535999e-05,  9.17357596e-05,  8.41544959e-09, -7.88783463e+00,\n",
       "        -1.77712393e-01,  4.05284149e-01]),\n",
       " array([ 4.80008114e-05,  7.84354421e-05,  6.15211858e-09, -8.04082717e+00,\n",
       "         1.96050717e-01,  3.36385547e-01]),\n",
       " array([ 5.07125430e-05,  8.04551470e-05,  6.47303068e-09, -8.01686576e+00,\n",
       "         2.65185659e-01,  7.19593140e-02]),\n",
       " array([ 5.11644069e-05,  6.80165654e-05,  4.62625317e-09, -8.17877380e+00,\n",
       "        -3.48503776e-02,  8.56857449e-02]),\n",
       " array([ 1.13151375e-04,  9.25298119e-04,  8.56176609e-07, -7.17546804e+00,\n",
       "         7.88315811e-01,  7.12434434e+00]),\n",
       " array([ 3.63324641e-05,  6.94635327e-05,  4.82518237e-09, -8.19486110e+00,\n",
       "        -9.90526481e-02,  6.82419677e+00]),\n",
       " array([-3.12501188e-05,  9.08763531e-05,  8.25851155e-09, -8.19846158e+00,\n",
       "         8.82565098e+00,  2.68631651e+02]),\n",
       " array([ 6.81844146e-06,  1.01176504e-04,  1.02366850e-08, -7.78404880e+00,\n",
       "        -1.59410970e-03, -3.26480265e-01]),\n",
       " array([ 2.46720156e-05,  8.59205654e-05,  7.38234355e-09,            -inf,\n",
       "        -1.34623893e-01,  6.75391353e-02]),\n",
       " array([ 5.10310396e-05,  1.03560905e-04,  1.07248610e-08,            -inf,\n",
       "        -1.08414302e+00,  4.26151816e+00]),\n",
       " array([ 4.41001689e-05,  6.62816589e-05,  4.39325831e-09, -8.20602742e+00,\n",
       "         1.41085314e-01, -1.60464133e-01]),\n",
       " array([ 2.03188349e-06,  6.91646508e-05,  4.78374893e-09, -8.17254237e+00,\n",
       "         3.12690500e-01,  6.88797512e-01]),\n",
       " array([ 1.41801665e-05,  6.56732232e-05,  4.31297224e-09, -8.21440755e+00,\n",
       "         5.63115117e-02,  2.60147587e-01])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mashh\\Documents\\git\\alzehimers_detection_using_eeg_graphs\\main.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mashh/Documents/git/alzehimers_detection_using_eeg_graphs/main.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:190\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    188\u001b[0m     check_consistent_length(X, y)\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    191\u001b[0m         X,\n\u001b[0;32m    192\u001b[0m         y,\n\u001b[0;32m    193\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[0;32m    194\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    195\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    196\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    197\u001b[0m     )\n\u001b[0;32m    199\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_targets(y)\n\u001b[0;32m    201\u001b[0m sample_weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\n\u001b[0;32m    202\u001b[0m     [] \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m sample_weight, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64\n\u001b[0;32m    203\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[1;32m-> 1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1147\u001b[0m     X,\n\u001b[0;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1149\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1150\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1151\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1152\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1153\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1154\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1155\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1156\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1157\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1158\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1160\u001b[0m )\n\u001b[0;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    952\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    954\u001b[0m         )\n\u001b[0;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 957\u001b[0m         _assert_all_finite(\n\u001b[0;32m    958\u001b[0m             array,\n\u001b[0;32m    959\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    960\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    961\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    964\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    965\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    123\u001b[0m     X,\n\u001b[0;32m    124\u001b[0m     xp\u001b[39m=\u001b[39;49mxp,\n\u001b[0;32m    125\u001b[0m     allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[0;32m    126\u001b[0m     msg_dtype\u001b[39m=\u001b[39;49mmsg_dtype,\n\u001b[0;32m    127\u001b[0m     estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    128\u001b[0m     input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    129\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    155\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    158\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n\u001b[1;32m--> 171\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
