{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, BatchNorm, global_add_pool, ChebConv, global_max_pool, SAGPooling, GATConv, GATv2Conv, TransformerConv, SuperGATConv, global_mean_pool, Linear\n",
    "from torch.nn import BatchNorm1d\n",
    "from math import floor\n",
    "import torch\n",
    "import torch_geometric\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGGraphConvNetLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduced_sensors=True, sfreq=None, batch_size=256, **kwargs):\n",
    "        super(EEGGraphConvNetLSTM, self).__init__()\n",
    "        # Define and initialize hyperparameters\n",
    "        self.sfreq = sfreq\n",
    "        self.batch_size = batch_size\n",
    "        #self.input_size = 8 if reduced_sensors else 62\n",
    "        \n",
    "        # Layers definition\n",
    "        input_size = kwargs.pop(\"input_size\", 1280)\n",
    "        hidden_dim = 320\n",
    "        \n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(input_size, 640, cached=True, normalize=False)\n",
    "        self.conv2 = GCNConv(640, 512, cached=True, normalize=False)\n",
    "        self.conv3 = GCNConv(512, 256, cached=True, normalize=False)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm1 = BatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm2 = BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm3 = BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batch_norm4 = BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.lstm = nn.LSTM(256, 256, 1, dropout=0.2)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        \n",
    "        # Xavier initializacion for fully connected layers\n",
    "        self.fc1.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc2.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc3.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weigth, batch):\n",
    "        \n",
    "        x = F.leaky_relu(self.batch_norm1(self.conv1(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.batch_norm2(self.conv2(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.batch_norm3(self.conv3(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        x, _ = self.lstm(x)\n",
    "        #x = F.leaky_relu(self.batch_norm4(self.conv4(x, edge_index, edge_weigth)), negative_slope=0.01)\n",
    "        #x = F.dropout(batch_norm_out, p=0.2, training=self.training)\n",
    "        \n",
    "        # Global add pooling\n",
    "        add_pool = global_add_pool(x, batch=batch)\n",
    "        # Apply fully connected layters\n",
    "        out = F.leaky_relu(self.fc1(add_pool), negative_slope=0.01)\n",
    "        out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        out = F.leaky_relu(self.fc2(out), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        out = F.leaky_relu(self.fc3(out))\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corrected_data_list(path):\n",
    "    data_list = list()\n",
    "    for file in path.iterdir():\n",
    "        data_list.append(torch.load(file))\n",
    "        \n",
    "    corrected_data_list = list()\n",
    "    \n",
    "    for data in data_list:\n",
    "    # print(data)\n",
    "        data = torch_geometric.data.Data(\n",
    "            x=torch.tensor(data.x),\n",
    "            edge_index=torch.tensor(data.edge_index),\n",
    "            edge_attr=torch.tensor(data.edge_attr),\n",
    "            label=torch.tensor(data.label),\n",
    "        )\n",
    "        corrected_data_list.append(data)\n",
    "        \n",
    "    rm = [\n",
    "      7,\n",
    "      14+1,\n",
    "      14+2,\n",
    "      17+3,\n",
    "      17+4,\n",
    "      26+5,\n",
    "      38+6,\n",
    "      54+7,\n",
    "      65+8,\n",
    "      69+9\n",
    "      ]\n",
    "\n",
    "    dl = list()\n",
    "    start = 0\n",
    "    for r in rm:\n",
    "        dl.extend(corrected_data_list[start:r])\n",
    "        start = r + 1\n",
    "\n",
    "    dl.extend(corrected_data_list[start:])\n",
    "    dl_filterd = list()\n",
    "    for data in dl:\n",
    "        if data.label == 2:\n",
    "            # print(data.label)\n",
    "            # if data.label == 2:\n",
    "            data.label = torch.tensor(1)\n",
    "        dl_filterd.append(data)\n",
    "\n",
    "    len(dl_filterd)\n",
    "    return dl_filterd\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_34968\\129136280.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x=torch.tensor(data.x),\n",
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_34968\\129136280.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[19, 153550], edge_index=[2, 361], edge_attr=[19, 19], label=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('graphs/raw_pearson/')\n",
    "\n",
    "\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=64, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=32, shuffle=False, num_workers=0)\n",
    "train_dl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = EEGGraphConvNetLSTM(input_size = 153550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 99303106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGGraphConvNetLSTM(\n",
       "  (conv1): GCNConv(153550, 640)\n",
       "  (conv2): GCNConv(640, 512)\n",
       "  (conv3): GCNConv(512, 256)\n",
       "  (batch_norm1): BatchNorm(640)\n",
       "  (batch_norm2): BatchNorm(512)\n",
       "  (batch_norm3): BatchNorm(256)\n",
       "  (batch_norm4): BatchNorm(256)\n",
       "  (lstm): LSTM(256, 256, dropout=0.2)\n",
       "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=0.518015\t acc=0.564103\n",
      "Test  : epoch=1\t loss=2.052299\t acc=0.500000\n",
      "\n",
      "Train : epoch=2\t loss=0.944445\t acc=0.653846\n",
      "Test  : epoch=2\t loss=0.947090\t acc=0.562500\n",
      "\n",
      "Train : epoch=3\t loss=0.362613\t acc=0.705128\n",
      "Test  : epoch=3\t loss=0.655671\t acc=0.562500\n",
      "\n",
      "Train : epoch=4\t loss=0.292389\t acc=0.717949\n",
      "Test  : epoch=4\t loss=0.801168\t acc=0.562500\n",
      "\n",
      "Train : epoch=5\t loss=0.301895\t acc=0.679487\n",
      "Test  : epoch=5\t loss=1.620729\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=0.382083\t acc=0.717949\n",
      "Test  : epoch=6\t loss=1.741892\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=0.269228\t acc=0.717949\n",
      "Test  : epoch=7\t loss=0.896446\t acc=0.562500\n",
      "\n",
      "Train : epoch=8\t loss=0.266489\t acc=0.846154\n",
      "Test  : epoch=8\t loss=1.073627\t acc=0.562500\n",
      "\n",
      "Train : epoch=9\t loss=0.228267\t acc=0.858974\n",
      "Test  : epoch=9\t loss=2.436531\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=0.362411\t acc=0.782051\n",
      "Test  : epoch=10\t loss=3.327008\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAW PLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_34968\\129136280.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x=torch.tensor(data.x),\n",
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_34968\\129136280.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/raw_pli/')\n",
    "\n",
    "\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=64, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = EEGGraphConvNetLSTM(input_size = 153550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 99303106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGGraphConvNetLSTM(\n",
       "  (conv1): GCNConv(153550, 640)\n",
       "  (conv2): GCNConv(640, 512)\n",
       "  (conv3): GCNConv(512, 256)\n",
       "  (batch_norm1): BatchNorm(640)\n",
       "  (batch_norm2): BatchNorm(512)\n",
       "  (batch_norm3): BatchNorm(256)\n",
       "  (batch_norm4): BatchNorm(256)\n",
       "  (lstm): LSTM(256, 256, dropout=0.2)\n",
       "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=0.944001\t acc=0.576923\n",
      "Test  : epoch=1\t loss=3.475220\t acc=0.500000\n",
      "\n",
      "Train : epoch=2\t loss=1.345836\t acc=0.641026\n",
      "Test  : epoch=2\t loss=1.562918\t acc=0.562500\n",
      "\n",
      "Train : epoch=3\t loss=0.454308\t acc=0.705128\n",
      "Test  : epoch=3\t loss=1.333666\t acc=0.500000\n",
      "\n",
      "Train : epoch=4\t loss=0.375936\t acc=0.692308\n",
      "Test  : epoch=4\t loss=0.563343\t acc=0.562500\n",
      "\n",
      "Train : epoch=5\t loss=0.244694\t acc=0.717949\n",
      "Test  : epoch=5\t loss=0.802312\t acc=0.562500\n",
      "\n",
      "Train : epoch=6\t loss=0.236556\t acc=0.730769\n",
      "Test  : epoch=6\t loss=1.034955\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=0.235329\t acc=0.705128\n",
      "Test  : epoch=7\t loss=0.887832\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=0.147699\t acc=0.833333\n",
      "Test  : epoch=8\t loss=1.294311\t acc=0.562500\n",
      "\n",
      "Train : epoch=9\t loss=0.129331\t acc=0.858974\n",
      "Test  : epoch=9\t loss=1.036173\t acc=0.687500\n",
      "\n",
      "Train : epoch=10\t loss=0.113313\t acc=0.948718\n",
      "Test  : epoch=10\t loss=0.663588\t acc=0.875000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86         8\n",
      "           1       0.80      1.00      0.89         8\n",
      "\n",
      "    accuracy                           0.88        16\n",
      "   macro avg       0.90      0.88      0.87        16\n",
      "weighted avg       0.90      0.88      0.87        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_34968\\129136280.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/moments_pearson/')\n",
    "\n",
    "\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=64, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = EEGGraphConvNetLSTM(input_size = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 1034946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGGraphConvNetLSTM(\n",
       "  (conv1): GCNConv(6, 640)\n",
       "  (conv2): GCNConv(640, 512)\n",
       "  (conv3): GCNConv(512, 256)\n",
       "  (batch_norm1): BatchNorm(640)\n",
       "  (batch_norm2): BatchNorm(512)\n",
       "  (batch_norm3): BatchNorm(256)\n",
       "  (batch_norm4): BatchNorm(256)\n",
       "  (lstm): LSTM(256, 256, dropout=0.2)\n",
       "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=0.846999\t acc=0.461538\n",
      "Test  : epoch=1\t loss=0.695920\t acc=0.500000\n",
      "\n",
      "Train : epoch=2\t loss=0.778846\t acc=0.692308\n",
      "Test  : epoch=2\t loss=0.695376\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=0.538867\t acc=0.653846\n",
      "Test  : epoch=3\t loss=3.931880\t acc=0.500000\n",
      "\n",
      "Train : epoch=4\t loss=0.777267\t acc=0.653846\n",
      "Test  : epoch=4\t loss=5.325059\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=0.906934\t acc=0.653846\n",
      "Test  : epoch=5\t loss=4.414628\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=0.905335\t acc=0.653846\n",
      "Test  : epoch=6\t loss=3.079530\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=0.737990\t acc=0.653846\n",
      "Test  : epoch=7\t loss=2.072034\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=0.515325\t acc=0.653846\n",
      "Test  : epoch=8\t loss=1.454544\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=0.419032\t acc=0.653846\n",
      "Test  : epoch=9\t loss=1.134950\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=0.400340\t acc=0.653846\n",
      "Test  : epoch=10\t loss=1.381717\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moments PLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_34968\\129136280.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/moments_pli/')\n",
    "\n",
    "\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=64, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EEGGraphConvNetLSTM(input_size = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 1034946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGGraphConvNetLSTM(\n",
       "  (conv1): GCNConv(6, 640)\n",
       "  (conv2): GCNConv(640, 512)\n",
       "  (conv3): GCNConv(512, 256)\n",
       "  (batch_norm1): BatchNorm(640)\n",
       "  (batch_norm2): BatchNorm(512)\n",
       "  (batch_norm3): BatchNorm(256)\n",
       "  (batch_norm4): BatchNorm(256)\n",
       "  (lstm): LSTM(256, 256, dropout=0.2)\n",
       "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=0.707861\t acc=0.397436\n",
      "Test  : epoch=1\t loss=0.696331\t acc=0.500000\n",
      "\n",
      "Train : epoch=2\t loss=0.598268\t acc=0.653846\n",
      "Test  : epoch=2\t loss=0.704921\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=0.809749\t acc=0.653846\n",
      "Test  : epoch=3\t loss=0.736039\t acc=0.500000\n",
      "\n",
      "Train : epoch=4\t loss=0.742485\t acc=0.653846\n",
      "Test  : epoch=4\t loss=0.766159\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=0.516999\t acc=0.653846\n",
      "Test  : epoch=5\t loss=0.905519\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=0.491121\t acc=0.653846\n",
      "Test  : epoch=6\t loss=1.580807\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=0.627055\t acc=0.653846\n",
      "Test  : epoch=7\t loss=1.896791\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=0.564187\t acc=0.653846\n",
      "Test  : epoch=8\t loss=2.133830\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=0.646898\t acc=0.653846\n",
      "Test  : epoch=9\t loss=2.197213\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=0.636284\t acc=0.653846\n",
      "Test  : epoch=10\t loss=1.784198\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSD CSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_34968\\129136280.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/psd_csd/')\n",
    "\n",
    "\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=64, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = EEGGraphConvNetLSTM(input_size = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 1034946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGGraphConvNetLSTM(\n",
       "  (conv1): GCNConv(6, 640)\n",
       "  (conv2): GCNConv(640, 512)\n",
       "  (conv3): GCNConv(512, 256)\n",
       "  (batch_norm1): BatchNorm(640)\n",
       "  (batch_norm2): BatchNorm(512)\n",
       "  (batch_norm3): BatchNorm(256)\n",
       "  (batch_norm4): BatchNorm(256)\n",
       "  (lstm): LSTM(256, 256, dropout=0.2)\n",
       "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=0.711846\t acc=0.525641\n",
      "Test  : epoch=1\t loss=0.693191\t acc=0.500000\n",
      "\n",
      "Train : epoch=2\t loss=0.672164\t acc=0.653846\n",
      "Test  : epoch=2\t loss=0.713102\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=0.585177\t acc=0.653846\n",
      "Test  : epoch=3\t loss=0.785440\t acc=0.500000\n",
      "\n",
      "Train : epoch=4\t loss=0.527329\t acc=0.653846\n",
      "Test  : epoch=4\t loss=0.859720\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=0.540956\t acc=0.653846\n",
      "Test  : epoch=5\t loss=0.914882\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=0.523301\t acc=0.653846\n",
      "Test  : epoch=6\t loss=0.963434\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=0.534079\t acc=0.653846\n",
      "Test  : epoch=7\t loss=0.978721\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=0.518691\t acc=0.653846\n",
      "Test  : epoch=8\t loss=0.977600\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=0.521367\t acc=0.653846\n",
      "Test  : epoch=9\t loss=0.984440\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=0.523277\t acc=0.653846\n",
      "Test  : epoch=10\t loss=0.986646\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSD Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashh\\AppData\\Local\\Temp\\ipykernel_34968\\129136280.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(data.edge_index),\n"
     ]
    }
   ],
   "source": [
    "path = Path('graphs/psd_pearson/')\n",
    "\n",
    "\n",
    "dl_filterd = create_corrected_data_list(path)\n",
    "\n",
    "train_dl, test_dl = train_test_split(dl_filterd, test_size=0.2, random_state=47744)\n",
    "train_dataloader = torch_geometric.loader.DataLoader(dl_filterd, batch_size=64, shuffle=False, num_workers=0)\n",
    "test_dataloader = torch_geometric.loader.DataLoader(test_dl, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = EEGGraphConvNetLSTM(input_size = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 1034946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGGraphConvNetLSTM(\n",
       "  (conv1): GCNConv(6, 640)\n",
       "  (conv2): GCNConv(640, 512)\n",
       "  (conv3): GCNConv(512, 256)\n",
       "  (batch_norm1): BatchNorm(640)\n",
       "  (batch_norm2): BatchNorm(512)\n",
       "  (batch_norm3): BatchNorm(256)\n",
       "  (batch_norm4): BatchNorm(256)\n",
       "  (lstm): LSTM(256, 256, dropout=0.2)\n",
       "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters())}\")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : epoch=1\t loss=0.757262\t acc=0.487179\n",
      "Test  : epoch=1\t loss=0.693176\t acc=0.500000\n",
      "\n",
      "Train : epoch=2\t loss=0.515620\t acc=0.666667\n",
      "Test  : epoch=2\t loss=1.730529\t acc=0.500000\n",
      "\n",
      "Train : epoch=3\t loss=0.813548\t acc=0.653846\n",
      "Test  : epoch=3\t loss=1.764509\t acc=0.500000\n",
      "\n",
      "Train : epoch=4\t loss=0.720682\t acc=0.653846\n",
      "Test  : epoch=4\t loss=1.806645\t acc=0.500000\n",
      "\n",
      "Train : epoch=5\t loss=0.566253\t acc=0.653846\n",
      "Test  : epoch=5\t loss=1.684644\t acc=0.500000\n",
      "\n",
      "Train : epoch=6\t loss=0.411043\t acc=0.653846\n",
      "Test  : epoch=6\t loss=1.673061\t acc=0.500000\n",
      "\n",
      "Train : epoch=7\t loss=0.330216\t acc=0.653846\n",
      "Test  : epoch=7\t loss=1.682278\t acc=0.500000\n",
      "\n",
      "Train : epoch=8\t loss=0.390329\t acc=0.653846\n",
      "Test  : epoch=8\t loss=1.622271\t acc=0.500000\n",
      "\n",
      "Train : epoch=9\t loss=0.283018\t acc=0.653846\n",
      "Test  : epoch=9\t loss=1.713523\t acc=0.500000\n",
      "\n",
      "Train : epoch=10\t loss=0.278640\t acc=0.653846\n",
      "Test  : epoch=10\t loss=2.141408\t acc=0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = False\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        #print(out.shape)\n",
    "        # print(data.label)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        acc = (out.argmax(dim=1) == data.label).sum().item() / len(data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "        # print(f'{count=}{out}')\n",
    "        # print(out.argmax(dim=1), data.label, end=' ')\n",
    "        if verbose: print(f\"epoch={epoch}\\t batch={idx+1} : loss={loss.item():.6f}\\t acc={acc:.6f}\", end='\\n')\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        count += 1\n",
    "    print(f\"Train : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n')\n",
    "\n",
    "    model.eval()\n",
    "    losses, y_true, y_pred = list(), list(), list()\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        # data = data[0]\n",
    "        # print(f\"{data[0]=}\")\n",
    "        # print(f\"{data.x.shape}\")\n",
    "    \n",
    "        \n",
    "        # out = model(x=data.x, edge_index=data.edge_index)\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_weigth=data.edge_attr, batch=data.batch)\n",
    "        # print(out.edge_attr)\n",
    "        # print(f\"{out.shape=}\")\n",
    "        # print(out)\n",
    "\n",
    "        loss = criterion(out, data.label)\n",
    "        losses.append(loss.item())\n",
    "        y_pred.extend(out.argmax(dim=1).tolist())\n",
    "        y_true.extend(data.label.tolist())\n",
    "\n",
    "    print(f\"Test  : epoch={epoch}\\t loss={sum(losses)/len(losses):.6f}\\t acc={accuracy_score(y_true, y_pred):.6f}\", end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.25      0.50      0.33        16\n",
      "weighted avg       0.25      0.50      0.33        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mashh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
